{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best so far but old features\n",
    "Use hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, matthews_corrcoef, accuracy_score, balanced_accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "tf.keras.utils.set_random_seed(SEED)  # This sets all random seeds in keras\n",
    "tf.config.experimental.enable_op_determinism()  # For complete reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prot_t5_data(pos_file, neg_file):\n",
    "    \"\"\"Load ProtT5 embeddings and align with existing data\"\"\"\n",
    "    # Read positive and negative files\n",
    "    pos_data = []\n",
    "    with open(pos_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(',')\n",
    "            entry = parts[0]\n",
    "            pos = int(parts[1])\n",
    "            embeddings = [float(x) for x in parts[2:]]\n",
    "            pos_data.append((entry, pos, embeddings))\n",
    "            \n",
    "    neg_data = []\n",
    "    with open(neg_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(',')\n",
    "            entry = parts[0]\n",
    "            pos = int(parts[1])\n",
    "            embeddings = [float(x) for x in parts[2:]]\n",
    "            neg_data.append((entry, pos, embeddings))\n",
    "    \n",
    "    # Convert to dictionaries for easy lookup\n",
    "    pos_dict = {(entry, pos): emb for entry, pos, emb in pos_data}\n",
    "    neg_dict = {(entry, pos): emb for entry, pos, emb in neg_data}\n",
    "    \n",
    "    return pos_dict, neg_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_aligned_data(seq_struct_df, pos_dict, neg_dict):\n",
    "    \"\"\"Align ProtT5 embeddings with sequence+structure data\"\"\"\n",
    "    embeddings = []\n",
    "    aligned_indices = []\n",
    "    \n",
    "    for idx, row in seq_struct_df.iterrows():\n",
    "        key = (row['entry'], row['pos'])\n",
    "        emb = pos_dict.get(key) if row['label'] == 1 else neg_dict.get(key)\n",
    "        \n",
    "        if emb is not None:\n",
    "            embeddings.append(emb)\n",
    "            aligned_indices.append(idx)\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    X_prot_t5 = np.array(embeddings)\n",
    "    \n",
    "    # Get aligned sequence+structure data\n",
    "    aligned_df = seq_struct_df.iloc[aligned_indices]\n",
    "    \n",
    "    return X_prot_t5, aligned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence_data(df):\n",
    "    \"\"\"Convert sequences to integer encoding\"\"\"\n",
    "    alphabet = 'ARNDCQEGHILKMFPSTWYV-'\n",
    "    char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "    \n",
    "    sequences = df['sequence'].values\n",
    "    encodings = []\n",
    "    \n",
    "    for seq in sequences:\n",
    "        try:\n",
    "            integer_encoded = [char_to_int[char] for char in seq]\n",
    "            encodings.append(integer_encoded)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sequence: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return np.array(encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_structure_data(df):\n",
    "    \"\"\"Enhanced feature preparation with better normalization\"\"\"\n",
    "    \n",
    "    # Normalize angles to their circular nature\n",
    "    def normalize_angles(angle_array):\n",
    "        angle_rad = np.pi * angle_array / 180.0\n",
    "        return np.stack([np.sin(angle_rad), np.cos(angle_rad)], axis=-1)\n",
    "    \n",
    "    # Process each feature type\n",
    "    features_list = []\n",
    "    \n",
    "    # 1. Process angles (phi, psi, omega, tau)\n",
    "    angles = ['phi', 'psi', 'omega', 'tau']\n",
    "    for angle in angles:\n",
    "        # Convert string to array\n",
    "        angle_arrays = np.array([np.array(eval(x)) for x in df[angle]])\n",
    "        # Get sin/cos representations\n",
    "        angle_features = normalize_angles(angle_arrays)\n",
    "        features_list.append(angle_features)\n",
    "    \n",
    "    # 2. Process SASA\n",
    "    sasa_arrays = np.array([np.array(eval(x)) for x in df['sasa']])\n",
    "    scaler = RobustScaler()\n",
    "    sasa_flat = sasa_arrays.reshape(-1, 1)\n",
    "    sasa_scaled = scaler.fit_transform(sasa_flat).reshape(sasa_arrays.shape)\n",
    "    features_list.append(sasa_scaled[..., np.newaxis])\n",
    "    \n",
    "    # 3. Process secondary structure\n",
    "    ss_arrays = np.array([list(seq) for seq in df['ss']])\n",
    "    ss_encoded = np.zeros((len(ss_arrays), ss_arrays.shape[1], 3))\n",
    "    ss_map = {'H': 0, 'E': 1, 'L': 2}\n",
    "    for i in range(len(ss_arrays)):\n",
    "        for j in range(len(ss_arrays[i])):\n",
    "            ss_encoded[i, j, ss_map[ss_arrays[i, j]]] = 1\n",
    "    features_list.append(ss_encoded)\n",
    "    \n",
    "    # Combine all features\n",
    "    features = np.concatenate(features_list, axis=-1)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tunable_model(hp, seq_length=33, struct_features=12, struct_window=0):\n",
    "    \"\"\"Creates a tunable version of the three-track model\"\"\"\n",
    "    \n",
    "    # Base regularization\n",
    "    l2_reg = hp.Float('l2_regularization', min_value=0.001, max_value=0.1, sampling='LOG')\n",
    "    regularizer = tf.keras.regularizers.l2(l2_reg)\n",
    "    \n",
    "    # Calculate structure window parameters\n",
    "    middle_pos = seq_length // 2\n",
    "    struct_positions = 1 + (2 * struct_window)\n",
    "    \n",
    "    # Sequence track - keeping embedding dim fixed at 21 to match reshape\n",
    "    seq_input = tf.keras.layers.Input(shape=(seq_length,), name='sequence_input')\n",
    "    x_seq = tf.keras.layers.Embedding(\n",
    "        21,  # vocabulary size\n",
    "        21,  # embedding dimension fixed to 21\n",
    "        input_length=seq_length\n",
    "    )(seq_input)\n",
    "    x_seq = tf.keras.layers.Reshape((seq_length, 21, 1))(x_seq)\n",
    "    \n",
    "    # Tunable Conv2D layer\n",
    "    x_seq = tf.keras.layers.Conv2D(\n",
    "        hp.Int('seq_conv_filters', min_value=16, max_value=64, step=16),\n",
    "        kernel_size=(hp.Int('seq_conv_kernel', min_value=15, max_value=19, step=2), 3),\n",
    "        activation='relu',\n",
    "        padding='valid'\n",
    "    )(x_seq)\n",
    "    x_seq = tf.keras.layers.BatchNormalization()(x_seq)\n",
    "    x_seq = tf.keras.layers.Dropout(\n",
    "        hp.Float('seq_dropout_1', min_value=0.2, max_value=0.5, step=0.1)\n",
    "    )(x_seq)\n",
    "    x_seq = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x_seq)\n",
    "    x_seq = tf.keras.layers.Flatten()(x_seq)\n",
    "    x_seq = tf.keras.layers.Dense(\n",
    "        hp.Int('seq_dense_units', min_value=16, max_value=64, step=16),\n",
    "        activation='relu',\n",
    "        kernel_regularizer=regularizer,\n",
    "        name='seq_features'\n",
    "    )(x_seq)\n",
    "    x_seq = tf.keras.layers.BatchNormalization()(x_seq)\n",
    "    x_seq = tf.keras.layers.Dropout(\n",
    "        hp.Float('seq_dropout_2', min_value=0.2, max_value=0.5, step=0.1)\n",
    "    )(x_seq)\n",
    "    \n",
    "    # Structure track\n",
    "    struct_input = tf.keras.layers.Input(shape=(seq_length, struct_features), name='structure_input')\n",
    "    \n",
    "    # Extract window around middle position\n",
    "    if struct_window == 0:\n",
    "        x_struct = tf.keras.layers.Lambda(\n",
    "            lambda x: x[:, middle_pos:middle_pos+1, :]\n",
    "        )(struct_input)\n",
    "    else:\n",
    "        x_struct = tf.keras.layers.Lambda(\n",
    "            lambda x: x[:, middle_pos-struct_window:middle_pos+struct_window+1, :]\n",
    "        )(struct_input)\n",
    "    \n",
    "    total_struct_features = struct_positions * struct_features\n",
    "    struct_dense_size = min(\n",
    "        total_struct_features * hp.Int('struct_dense_multiplier', min_value=1, max_value=4),\n",
    "        hp.Int('max_struct_dense', min_value=64, max_value=256, step=64)\n",
    "    )\n",
    "    \n",
    "    x_struct = tf.keras.layers.Conv1D(\n",
    "        hp.Int('struct_conv_filters', min_value=16, max_value=64, step=16),\n",
    "        hp.Int('struct_conv_kernel', min_value=2, max_value=4),\n",
    "        padding='same',\n",
    "        activation='relu'\n",
    "    )(x_struct)\n",
    "    x_struct = tf.keras.layers.BatchNormalization()(x_struct)\n",
    "    x_struct = tf.keras.layers.Dropout(\n",
    "        hp.Float('struct_dropout_1', min_value=0.1, max_value=0.4, step=0.1)\n",
    "    )(x_struct)\n",
    "    x_struct = tf.keras.layers.Flatten()(x_struct)\n",
    "    x_struct = tf.keras.layers.Dense(struct_dense_size, activation='relu')(x_struct)\n",
    "    x_struct = tf.keras.layers.Dropout(\n",
    "        hp.Float('struct_dropout_2', min_value=0.1, max_value=0.4, step=0.1)\n",
    "    )(x_struct)\n",
    "    \n",
    "    # ProtT5 track\n",
    "    prot_t5_input = tf.keras.layers.Input(shape=(1024,), name='prot_t5_input')\n",
    "    x_prot_t5 = tf.keras.layers.Dense(\n",
    "        hp.Int('prot_t5_dense_1', min_value=128, max_value=512, step=128),\n",
    "        kernel_regularizer=regularizer\n",
    "    )(prot_t5_input)\n",
    "    x_prot_t5 = tf.keras.layers.BatchNormalization()(x_prot_t5)\n",
    "    x_prot_t5 = tf.keras.layers.Dropout(\n",
    "        hp.Float('prot_t5_dropout_1', min_value=0.3, max_value=0.6, step=0.1)\n",
    "    )(x_prot_t5)\n",
    "    x_prot_t5 = tf.keras.layers.Dense(\n",
    "        hp.Int('prot_t5_dense_2', min_value=64, max_value=256, step=64),\n",
    "        activation='relu',\n",
    "        kernel_regularizer=regularizer\n",
    "    )(x_prot_t5)\n",
    "    x_prot_t5 = tf.keras.layers.BatchNormalization()(x_prot_t5)\n",
    "    x_prot_t5 = tf.keras.layers.Dropout(\n",
    "        hp.Float('prot_t5_dropout_2', min_value=0.3, max_value=0.6, step=0.1)\n",
    "    )(x_prot_t5)\n",
    "    \n",
    "    # Create learnable weights layer\n",
    "    weight_layer = tf.keras.layers.Dense(3, activation='softmax', name='track_weights')\n",
    "    track_weights = weight_layer(tf.keras.layers.Concatenate()([x_seq, x_struct, x_prot_t5]))\n",
    "    \n",
    "    # Apply weights\n",
    "    weighted_seq = tf.keras.layers.Multiply(name='weighted_seq')([\n",
    "        x_seq,\n",
    "        tf.keras.layers.Lambda(lambda x: x[:, 0:1])(track_weights)\n",
    "    ])\n",
    "    \n",
    "    weighted_struct = tf.keras.layers.Multiply(name='weighted_struct')([\n",
    "        x_struct,\n",
    "        tf.keras.layers.Lambda(lambda x: x[:, 1:2])(track_weights)\n",
    "    ])\n",
    "    \n",
    "    weighted_prot_t5 = tf.keras.layers.Multiply(name='weighted_prot_t5')([\n",
    "        x_prot_t5,\n",
    "        tf.keras.layers.Lambda(lambda x: x[:, 2:3])(track_weights)\n",
    "    ])\n",
    "    \n",
    "    # Combine features\n",
    "    combined = tf.keras.layers.Concatenate()([weighted_seq, weighted_struct, weighted_prot_t5])\n",
    "    \n",
    "    # Final layers\n",
    "    x = tf.keras.layers.Dense(\n",
    "        hp.Int('final_dense_1', min_value=32, max_value=128, step=32),\n",
    "        activation='relu',\n",
    "        kernel_regularizer=regularizer\n",
    "    )(combined)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(\n",
    "        hp.Float('final_dropout_1', min_value=0.3, max_value=0.6, step=0.1)\n",
    "    )(x)\n",
    "    x = tf.keras.layers.Dense(\n",
    "        hp.Int('final_dense_2', min_value=16, max_value=64, step=16),\n",
    "        activation='relu',\n",
    "        kernel_regularizer=regularizer\n",
    "    )(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(\n",
    "        hp.Float('final_dropout_2', min_value=0.3, max_value=0.6, step=0.1)\n",
    "    )(x)\n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[seq_input, struct_input, prot_t5_input], outputs=outputs)\n",
    "    \n",
    "    # Add learning rate to hyperparameter search\n",
    "    learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Add track weights getter\n",
    "    def get_track_weights():\n",
    "        w = model.get_layer('track_weights').get_weights()\n",
    "        if len(w) > 0:\n",
    "            weights = w[0]\n",
    "            bias = w[1] if len(w) > 1 else 0\n",
    "            exp_weights = np.exp(np.mean(weights, axis=0) + bias)\n",
    "            normalized = exp_weights / np.sum(exp_weights)\n",
    "            return {\n",
    "                'sequence': float(normalized[0]),\n",
    "                'structure': float(normalized[1]),\n",
    "                'prot_t5': float(normalized[2])\n",
    "            }\n",
    "        return {'sequence': 0.33, 'structure': 0.33, 'prot_t5': 0.34}\n",
    "    \n",
    "    model.get_track_weights = get_track_weights\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(struct_window=0):\n",
    "    \"\"\"Training function with track weights\"\"\"\n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    train_df = pd.read_csv(\"../data/old data/processed_data_train_after.csv\")\n",
    "    test_df = pd.read_csv(\"../data/old data/processed_data_test_after.csv\")\n",
    "    \n",
    "    # Load ProtT5 embeddings\n",
    "    print(\"Loading ProtT5 embeddings...\")\n",
    "    train_pos_dict, train_neg_dict = load_prot_t5_data(\n",
    "        '/Users/hai/Workspace/UniFreiburg/WS2024/Thesis/LMSuccSite/data/train/features/train_positive_ProtT5-XL-UniRef50.csv',\n",
    "        '/Users/hai/Workspace/UniFreiburg/WS2024/Thesis/LMSuccSite/data/train/features/train_negative_ProtT5-XL-UniRef50.csv'\n",
    "    )\n",
    "    test_pos_dict, test_neg_dict = load_prot_t5_data(\n",
    "        '/Users/hai/Workspace/UniFreiburg/WS2024/Thesis/LMSuccSite/data/test/features/test_positive_ProtT5-XL-UniRef50.csv',\n",
    "        '/Users/hai/Workspace/UniFreiburg/WS2024/Thesis/LMSuccSite/data/test/features/test_negative_ProtT5-XL-UniRef50.csv'\n",
    "    )\n",
    "    \n",
    "    # Align data\n",
    "    print(\"Aligning data...\")\n",
    "    X_train_prot_t5, train_df_aligned = prepare_aligned_data(train_df, train_pos_dict, train_neg_dict)\n",
    "    X_test_prot_t5, test_df_aligned = prepare_aligned_data(test_df, test_pos_dict, test_neg_dict)\n",
    "    \n",
    "    # Prepare other data\n",
    "    X_train_seq = prepare_sequence_data(train_df_aligned)\n",
    "    X_test_seq = prepare_sequence_data(test_df_aligned)\n",
    "    X_train_struct = prepare_structure_data(train_df_aligned)\n",
    "    X_test_struct = prepare_structure_data(test_df_aligned)\n",
    "    \n",
    "    y_train = train_df_aligned['label'].values\n",
    "    y_test = test_df_aligned['label'].values\n",
    "    \n",
    "    # Shuffle training data\n",
    "    shuffle_idx = np.random.RandomState(42).permutation(len(y_train))\n",
    "    X_train_seq = X_train_seq[shuffle_idx]\n",
    "    X_train_struct = X_train_struct[shuffle_idx]\n",
    "    X_train_prot_t5 = X_train_prot_t5[shuffle_idx]\n",
    "    y_train = y_train[shuffle_idx]\n",
    "    \n",
    "    # Shuffle test data\n",
    "    shuffle_idx_test = np.random.RandomState(42).permutation(len(y_test))\n",
    "    X_test_seq = X_test_seq[shuffle_idx_test]\n",
    "    X_test_struct = X_test_struct[shuffle_idx_test]\n",
    "    X_test_prot_t5 = X_test_prot_t5[shuffle_idx_test]\n",
    "    y_test = y_test[shuffle_idx_test]\n",
    "    \n",
    "    # Print class distribution after shuffling\n",
    "    print(\"\\nTraining set distribution (after shuffling):\")\n",
    "    print(pd.Series(y_train).value_counts())\n",
    "    print(\"\\nTest set distribution (after shuffling):\")\n",
    "    print(pd.Series(y_test).value_counts())\n",
    "    \n",
    "    # Print data shapes\n",
    "    print(\"\\nData shapes:\")\n",
    "    print(f\"X_train_seq: {X_train_seq.shape}\")\n",
    "    print(f\"X_train_struct: {X_train_struct.shape}\")\n",
    "    print(f\"X_train_prot_t5: {X_train_prot_t5.shape}\")\n",
    "    print(f\"X_test_seq: {X_test_seq.shape}\")\n",
    "    print(f\"X_test_struct: {X_test_struct.shape}\")\n",
    "    print(f\"X_test_prot_t5: {X_test_prot_t5.shape}\")\n",
    "    print(f\"y_train: {y_train.shape}\")\n",
    "    print(f\"y_test: {y_test.shape}\")\n",
    "    \n",
    "    # Calculate class weights\n",
    "    total_samples = len(y_train)\n",
    "    pos_samples = np.sum(y_train == 1)\n",
    "    neg_samples = np.sum(y_train == 0)\n",
    "    \n",
    "    class_weights = {\n",
    "        0: total_samples / (2 * neg_samples),\n",
    "        1: total_samples / (2 * pos_samples)\n",
    "    }\n",
    "    \n",
    "    # Cross-validation\n",
    "    kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    metrics = {'acc': [], 'balanced_acc': [], 'mcc': [], 'sn': [], 'sp': []}\n",
    "    test_predictions = []\n",
    "    \n",
    "    track_weights_history = []\n",
    "    best_hp_per_fold = []\n",
    "    \n",
    "    tuner = kt.Hyperband(\n",
    "        lambda hp: create_tunable_model(\n",
    "            hp, \n",
    "            seq_length=33, \n",
    "            struct_features=X_train_struct.shape[2], \n",
    "            struct_window=struct_window\n",
    "        ),\n",
    "        objective=kt.Objective('val_accuracy', direction='max'),\n",
    "        max_epochs=50,\n",
    "        factor=3,\n",
    "        directory='hyperparameter_search',\n",
    "        project_name='succinylation_prediction'\n",
    "    )\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train_seq), 1):\n",
    "        print(f\"\\nFold {fold}/5\")\n",
    "        \n",
    "        # First, search for best hyperparameters\n",
    "        print(\"\\nSearching for best hyperparameters...\")\n",
    "        tuner.search(\n",
    "            [X_train_seq[train_idx], X_train_struct[train_idx], X_train_prot_t5[train_idx]],\n",
    "            y_train[train_idx],\n",
    "            validation_data=(\n",
    "                [X_train_seq[val_idx], X_train_struct[val_idx], X_train_prot_t5[val_idx]],\n",
    "                y_train[val_idx]\n",
    "            ),\n",
    "            epochs=50,\n",
    "            batch_size=32,\n",
    "            callbacks=[\n",
    "                tf.keras.callbacks.EarlyStopping(\n",
    "                    monitor='val_loss',\n",
    "                    patience=5,\n",
    "                    restore_best_weights=True\n",
    "                )\n",
    "            ],\n",
    "            class_weight=class_weights\n",
    "        )\n",
    "        \n",
    "        # Get best hyperparameters for this fold\n",
    "        best_hp = tuner.get_best_hyperparameters(1)[0]\n",
    "        best_hp_per_fold.append(best_hp)\n",
    "        print(\"\\nBest hyperparameters for this fold:\", best_hp.values)\n",
    "        \n",
    "        # Build model with best hyperparameters\n",
    "        model = tuner.hypermodel.build(best_hp)\n",
    "        \n",
    "        # Train final model with best hyperparameters\n",
    "        print(\"\\nTraining final model with best hyperparameters...\")\n",
    "        history = model.fit(\n",
    "            [X_train_seq[train_idx], X_train_struct[train_idx], X_train_prot_t5[train_idx]],\n",
    "            y_train[train_idx],\n",
    "            validation_data=(\n",
    "                [X_train_seq[val_idx], X_train_struct[val_idx], X_train_prot_t5[val_idx]],\n",
    "                y_train[val_idx]\n",
    "            ),\n",
    "            batch_size=32,\n",
    "            epochs=50,\n",
    "            callbacks=[\n",
    "                tf.keras.callbacks.EarlyStopping(\n",
    "                    monitor='val_loss',\n",
    "                    patience=5,\n",
    "                    restore_best_weights=True\n",
    "                ),\n",
    "                tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                    monitor='val_loss',\n",
    "                    factor=0.5,\n",
    "                    patience=3,\n",
    "                    min_lr=1e-6\n",
    "                )\n",
    "            ],\n",
    "            class_weight=class_weights,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Plot training history\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.title(f'Model Accuracy - Fold {fold}')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        y_pred = model.predict([X_train_seq[val_idx], X_train_struct[val_idx], X_train_prot_t5[val_idx]])\n",
    "        y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        cm = confusion_matrix(y_train[val_idx], y_pred_binary)\n",
    "        metrics['acc'].append(accuracy_score(y_train[val_idx], y_pred_binary))\n",
    "        metrics['balanced_acc'].append(balanced_accuracy_score(y_train[val_idx], y_pred_binary))\n",
    "        metrics['mcc'].append(matthews_corrcoef(y_train[val_idx], y_pred_binary))\n",
    "        metrics['sn'].append(cm[1][1]/(cm[1][1]+cm[1][0]))  # Sensitivity\n",
    "        metrics['sp'].append(cm[0][0]/(cm[0][0]+cm[0][1]))  # Specificity\n",
    "        \n",
    "        # Predict on test set\n",
    "        test_pred = model.predict([X_test_seq, X_test_struct, X_test_prot_t5])\n",
    "        test_predictions.append(test_pred)\n",
    "        \n",
    "        # Print fold results\n",
    "        print(f\"\\nFold {fold} Results:\")\n",
    "        print(f\"Accuracy: {metrics['acc'][-1]:.4f}\")\n",
    "        print(f\"Balanced Accuracy: {metrics['balanced_acc'][-1]:.4f}\")\n",
    "        print(f\"MCC: {metrics['mcc'][-1]:.4f}\")\n",
    "        print(f\"Sensitivity: {metrics['sn'][-1]:.4f}\")\n",
    "        print(f\"Specificity: {metrics['sp'][-1]:.4f}\")\n",
    "        \n",
    "        # After training, get and store the learned weights\n",
    "        final_weights = model.get_track_weights()\n",
    "        track_weights_history.append(final_weights)\n",
    "        print(f\"\\nLearned track weights for fold {fold}:\")\n",
    "        print(f\"Sequence weight: {final_weights['sequence']:.4f}\")\n",
    "        print(f\"Structure weight: {final_weights['structure']:.4f}\")\n",
    "        print(f\"ProtT5 weight: {final_weights['prot_t5']:.4f}\")\n",
    "        \n",
    "        # Clear session to free memory\n",
    "        tf.keras.backend.clear_session()\n",
    "    \n",
    "    # Print summary of best hyperparameters across folds\n",
    "    print(\"\\nBest hyperparameters summary across folds:\")\n",
    "    for i, hp in enumerate(best_hp_per_fold, 1):\n",
    "        print(f\"\\nFold {i} best hyperparameters:\")\n",
    "        for param, value in hp.values.items():\n",
    "            print(f\"{param}: {value}\")\n",
    "    \n",
    "    # Calculate average weights\n",
    "    avg_seq_weight = np.mean([w['sequence'] for w in track_weights_history])\n",
    "    avg_struct_weight = np.mean([w['structure'] for w in track_weights_history])\n",
    "    avg_prot_t5_weight = np.mean([w['prot_t5'] for w in track_weights_history])\n",
    "    std_seq_weight = np.std([w['sequence'] for w in track_weights_history])\n",
    "    std_struct_weight = np.std([w['structure'] for w in track_weights_history])\n",
    "    std_prot_t5_weight = np.std([w['prot_t5'] for w in track_weights_history])\n",
    "    \n",
    "    print(\"\\nAverage track weights across folds:\")\n",
    "    print(f\"Sequence weight: {avg_seq_weight:.4f} ± {std_seq_weight:.4f}\")\n",
    "    print(f\"Structure weight: {avg_struct_weight:.4f} ± {std_struct_weight:.4f}\")\n",
    "    print(f\"ProtT5 weight: {avg_prot_t5_weight:.4f} ± {std_prot_t5_weight:.4f}\")\n",
    "    \n",
    "    # Print average cross-validation results\n",
    "    print(\"\\nAverage Cross-validation Results:\")\n",
    "    for metric in metrics:\n",
    "        print(f\"{metric.upper()}: {np.mean(metrics[metric]):.4f} ± {np.std(metrics[metric]):.4f}\")\n",
    "    \n",
    "    # Ensemble predictions on test set\n",
    "    test_pred_avg = np.mean(test_predictions, axis=0)\n",
    "    test_pred_binary = (test_pred_avg > 0.5).astype(int)\n",
    "    \n",
    "    # Calculate final test metrics\n",
    "    cm_test = confusion_matrix(y_test, test_pred_binary)\n",
    "    \n",
    "    print(\"\\nFinal Test Set Results:\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, test_pred_binary):.4f}\")\n",
    "    print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, test_pred_binary):.4f}\")\n",
    "    print(f\"MCC: {matthews_corrcoef(y_test, test_pred_binary):.4f}\")\n",
    "    print(f\"Sensitivity: {cm_test[1][1]/(cm_test[1][1]+cm_test[1][0]):.4f}\")\n",
    "    print(f\"Specificity: {cm_test[0][0]/(cm_test[0][0]+cm_test[0][1]):.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm_test)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2 Complete [00h 00m 43s]\n",
      "\n",
      "Best val_accuracy So Far: None\n",
      "Total elapsed time: 00h 01m 19s\n",
      "\n",
      "Search: Running Trial #3\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "0.0010353         |0.013843          |l2_regularization\n",
      "16                |32                |seq_conv_filters\n",
      "19                |19                |seq_conv_kernel\n",
      "0.4               |0.2               |seq_dropout_1\n",
      "16                |16                |seq_dense_units\n",
      "0.4               |0.4               |seq_dropout_2\n",
      "3                 |4                 |struct_dense_multiplier\n",
      "64                |192               |max_struct_dense\n",
      "32                |64                |struct_conv_filters\n",
      "3                 |4                 |struct_conv_kernel\n",
      "0.2               |0.2               |struct_dropout_1\n",
      "0.1               |0.4               |struct_dropout_2\n",
      "384               |256               |prot_t5_dense_1\n",
      "0.5               |0.3               |prot_t5_dropout_1\n",
      "64                |128               |prot_t5_dense_2\n",
      "0.5               |0.4               |prot_t5_dropout_2\n",
      "96                |32                |final_dense_1\n",
      "0.5               |0.3               |final_dropout_1\n",
      "64                |16                |final_dense_2\n",
      "0.3               |0.3               |final_dropout_2\n",
      "0.0045522         |0.008036          |learning_rate\n",
      "2                 |2                 |tuner/epochs\n",
      "0                 |0                 |tuner/initial_epoch\n",
      "3                 |3                 |tuner/bracket\n",
      "0                 |0                 |tuner/round\n",
      "\n",
      "Epoch 1/2\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.5572 - loss: 1.8595"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/hai/miniconda3/envs/bioinf/lib/python3.9/site-packages/keras_tuner/src/engine/base_tuner.py\", line 274, in _try_run_and_update_trial\n",
      "    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n",
      "  File \"/Users/hai/miniconda3/envs/bioinf/lib/python3.9/site-packages/keras_tuner/src/engine/base_tuner.py\", line 239, in _run_and_update_trial\n",
      "    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n",
      "  File \"/Users/hai/miniconda3/envs/bioinf/lib/python3.9/site-packages/keras_tuner/src/tuners/hyperband.py\", line 427, in run_trial\n",
      "    return super().run_trial(trial, *fit_args, **fit_kwargs)\n",
      "  File \"/Users/hai/miniconda3/envs/bioinf/lib/python3.9/site-packages/keras_tuner/src/engine/tuner.py\", line 314, in run_trial\n",
      "    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n",
      "  File \"/Users/hai/miniconda3/envs/bioinf/lib/python3.9/site-packages/keras_tuner/src/engine/tuner.py\", line 233, in _build_and_fit_model\n",
      "    results = self.hypermodel.fit(hp, model, *args, **kwargs)\n",
      "  File \"/Users/hai/miniconda3/envs/bioinf/lib/python3.9/site-packages/keras_tuner/src/engine/hypermodel.py\", line 149, in fit\n",
      "    return model.fit(*args, **kwargs)\n",
      "  File \"/Users/hai/miniconda3/envs/bioinf/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"/Users/hai/miniconda3/envs/bioinf/lib/python3.9/site-packages/keras_tuner/src/engine/tuner_utils.py\", line 76, in on_epoch_end\n",
      "    self._save_model()\n",
      "  File \"/Users/hai/miniconda3/envs/bioinf/lib/python3.9/site-packages/keras_tuner/src/engine/tuner_utils.py\", line 86, in _save_model\n",
      "    self.model.save_weights(write_filepath)\n",
      "  File \"/Users/hai/miniconda3/envs/bioinf/lib/python3.9/site-packages/h5py/_hl/group.py\", line 483, in __setitem__\n",
      "    ds = self.create_dataset(None, data=obj)\n",
      "  File \"/Users/hai/miniconda3/envs/bioinf/lib/python3.9/site-packages/h5py/_hl/group.py\", line 183, in create_dataset\n",
      "    dsid = dataset.make_new_dset(group, shape, dtype, data, name, **kwds)\n",
      "  File \"/Users/hai/miniconda3/envs/bioinf/lib/python3.9/site-packages/h5py/_hl/dataset.py\", line 86, in make_new_dset\n",
      "    tid = h5t.py_create(dtype, logical=1)\n",
      "  File \"h5py/h5t.pyx\", line 1663, in h5py.h5t.py_create\n",
      "  File \"h5py/h5t.pyx\", line 1687, in h5py.h5t.py_create\n",
      "  File \"h5py/h5t.pyx\", line 1705, in h5py.h5t.py_create\n",
      "  File \"h5py/h5t.pyx\", line 1459, in h5py.h5t._c_int\n",
      "TypeError: Unsupported integer size (0)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Number of consecutive failures exceeded the limit of 3.\nTraceback (most recent call last):\n  File \"/Users/hai/miniconda3/envs/bioinf/lib/python3.9/site-packages/keras_tuner/src/engine/base_tuner.py\", line 274, in _try_run_and_update_trial\n    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n  File \"/Users/hai/miniconda3/envs/bioinf/lib/python3.9/site-packages/keras_tuner/src/engine/base_tuner.py\", line 239, in _run_and_update_trial\n    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n  File \"/Users/hai/miniconda3/envs/bioinf/lib/python3.9/site-packages/keras_tuner/src/tuners/hyperband.py\", line 427, in run_trial\n    return super().run_trial(trial, *fit_args, **fit_kwargs)\n  File \"/Users/hai/miniconda3/envs/bioinf/lib/python3.9/site-packages/keras_tuner/src/engine/tuner.py\", line 314, in run_trial\n    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n  File \"/Users/hai/miniconda3/envs/bioinf/lib/python3.9/site-packages/keras_tuner/src/engine/tuner.py\", line 233, in _build_and_fit_model\n    results = self.hypermodel.fit(hp, model, *args, **kwargs)\n  File \"/Users/hai/miniconda3/envs/bioinf/lib/python3.9/site-packages/keras_tuner/src/engine/hypermodel.py\", line 149, in fit\n    return model.fit(*args, **kwargs)\n  File \"/Users/hai/miniconda3/envs/bioinf/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"/Users/hai/miniconda3/envs/bioinf/lib/python3.9/site-packages/keras_tuner/src/engine/tuner_utils.py\", line 76, in on_epoch_end\n    self._save_model()\n  File \"/Users/hai/miniconda3/envs/bioinf/lib/python3.9/site-packages/keras_tuner/src/engine/tuner_utils.py\", line 86, in _save_model\n    self.model.save_weights(write_filepath)\n  File \"/Users/hai/miniconda3/envs/bioinf/lib/python3.9/site-packages/h5py/_hl/group.py\", line 483, in __setitem__\n    ds = self.create_dataset(None, data=obj)\n  File \"/Users/hai/miniconda3/envs/bioinf/lib/python3.9/site-packages/h5py/_hl/group.py\", line 183, in create_dataset\n    dsid = dataset.make_new_dset(group, shape, dtype, data, name, **kwds)\n  File \"/Users/hai/miniconda3/envs/bioinf/lib/python3.9/site-packages/h5py/_hl/dataset.py\", line 86, in make_new_dset\n    tid = h5t.py_create(dtype, logical=1)\n  File \"h5py/h5t.pyx\", line 1663, in h5py.h5t.py_create\n  File \"h5py/h5t.pyx\", line 1687, in h5py.h5t.py_create\n  File \"h5py/h5t.pyx\", line 1705, in h5py.h5t.py_create\n  File \"h5py/h5t.pyx\", line 1459, in h5py.h5t._c_int\nTypeError: Unsupported integer size (0)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model, best_hyperparameters \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstruct_window\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 101\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(struct_window)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# First, search for best hyperparameters\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSearching for best hyperparameters...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 101\u001b[0m \u001b[43mtuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_train_seq\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_struct\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_prot_t5\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_train_seq\u001b[49m\u001b[43m[\u001b[49m\u001b[43mval_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_struct\u001b[49m\u001b[43m[\u001b[49m\u001b[43mval_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_prot_t5\u001b[49m\u001b[43m[\u001b[49m\u001b[43mval_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mval_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrestore_best_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weights\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# Get best hyperparameters for this fold\u001b[39;00m\n\u001b[1;32m    121\u001b[0m best_hp \u001b[38;5;241m=\u001b[39m tuner\u001b[38;5;241m.\u001b[39mget_best_hyperparameters(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/bioinf/lib/python3.9/site-packages/keras_tuner/src/engine/base_tuner.py:235\u001b[0m, in \u001b[0;36mBaseTuner.search\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_begin(trial)\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_run_and_update_trial(trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs)\n\u001b[0;32m--> 235\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_trial_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_search_end()\n",
      "File \u001b[0;32m~/miniconda3/envs/bioinf/lib/python3.9/site-packages/keras_tuner/src/engine/base_tuner.py:339\u001b[0m, in \u001b[0;36mBaseTuner.on_trial_end\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_trial_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial):\n\u001b[1;32m    334\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Called at the end of a trial.\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;124;03m        trial: A `Trial` instance.\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 339\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moracle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave()\n",
      "File \u001b[0;32m~/miniconda3/envs/bioinf/lib/python3.9/site-packages/keras_tuner/src/engine/oracle.py:108\u001b[0m, in \u001b[0;36msynchronized.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m     LOCKS[oracle]\u001b[38;5;241m.\u001b[39macquire()\n\u001b[1;32m    107\u001b[0m     THREADS[oracle] \u001b[38;5;241m=\u001b[39m thread_name\n\u001b[0;32m--> 108\u001b[0m ret_val \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m need_acquire:\n\u001b[1;32m    110\u001b[0m     THREADS[oracle] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/bioinf/lib/python3.9/site-packages/keras_tuner/src/engine/oracle.py:588\u001b[0m, in \u001b[0;36mOracle.end_trial\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry(trial):\n\u001b[1;32m    587\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend_order\u001b[38;5;241m.\u001b[39mappend(trial\u001b[38;5;241m.\u001b[39mtrial_id)\n\u001b[0;32m--> 588\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_consecutive_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_trial(trial)\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave()\n",
      "File \u001b[0;32m~/miniconda3/envs/bioinf/lib/python3.9/site-packages/keras_tuner/src/engine/oracle.py:545\u001b[0m, in \u001b[0;36mOracle._check_consecutive_failures\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    543\u001b[0m     consecutive_failures \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m consecutive_failures \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_consecutive_failed_trials:\n\u001b[0;32m--> 545\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    546\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of consecutive failures exceeded the limit \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_consecutive_failed_trials\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    548\u001b[0m         \u001b[38;5;241m+\u001b[39m (trial\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    549\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Number of consecutive failures exceeded the limit of 3.\nTraceback (most recent call last):\n  File \"/Users/hai/miniconda3/envs/bioinf/lib/python3.9/site-packages/keras_tuner/src/engine/base_tuner.py\", line 274, in _try_run_and_update_trial\n    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n  File \"/Users/hai/miniconda3/envs/bioinf/lib/python3.9/site-packages/keras_tuner/src/engine/base_tuner.py\", line 239, in _run_and_update_trial\n    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n  File \"/Users/hai/miniconda3/envs/bioinf/lib/python3.9/site-packages/keras_tuner/src/tuners/hyperband.py\", line 427, in run_trial\n    return super().run_trial(trial, *fit_args, **fit_kwargs)\n  File \"/Users/hai/miniconda3/envs/bioinf/lib/python3.9/site-packages/keras_tuner/src/engine/tuner.py\", line 314, in run_trial\n    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n  File \"/Users/hai/miniconda3/envs/bioinf/lib/python3.9/site-packages/keras_tuner/src/engine/tuner.py\", line 233, in _build_and_fit_model\n    results = self.hypermodel.fit(hp, model, *args, **kwargs)\n  File \"/Users/hai/miniconda3/envs/bioinf/lib/python3.9/site-packages/keras_tuner/src/engine/hypermodel.py\", line 149, in fit\n    return model.fit(*args, **kwargs)\n  File \"/Users/hai/miniconda3/envs/bioinf/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"/Users/hai/miniconda3/envs/bioinf/lib/python3.9/site-packages/keras_tuner/src/engine/tuner_utils.py\", line 76, in on_epoch_end\n    self._save_model()\n  File \"/Users/hai/miniconda3/envs/bioinf/lib/python3.9/site-packages/keras_tuner/src/engine/tuner_utils.py\", line 86, in _save_model\n    self.model.save_weights(write_filepath)\n  File \"/Users/hai/miniconda3/envs/bioinf/lib/python3.9/site-packages/h5py/_hl/group.py\", line 483, in __setitem__\n    ds = self.create_dataset(None, data=obj)\n  File \"/Users/hai/miniconda3/envs/bioinf/lib/python3.9/site-packages/h5py/_hl/group.py\", line 183, in create_dataset\n    dsid = dataset.make_new_dset(group, shape, dtype, data, name, **kwds)\n  File \"/Users/hai/miniconda3/envs/bioinf/lib/python3.9/site-packages/h5py/_hl/dataset.py\", line 86, in make_new_dset\n    tid = h5t.py_create(dtype, logical=1)\n  File \"h5py/h5t.pyx\", line 1663, in h5py.h5t.py_create\n  File \"h5py/h5t.pyx\", line 1687, in h5py.h5t.py_create\n  File \"h5py/h5t.pyx\", line 1705, in h5py.h5t.py_create\n  File \"h5py/h5t.pyx\", line 1459, in h5py.h5t._c_int\nTypeError: Unsupported integer size (0)\n"
     ]
    }
   ],
   "source": [
    "model, best_hyperparameters = train_and_evaluate(struct_window=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioinf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
