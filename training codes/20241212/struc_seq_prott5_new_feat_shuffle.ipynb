{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This add normalization, best result so far\n",
    "Also add suffling to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, matthews_corrcoef, accuracy_score, balanced_accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "tf.keras.utils.set_random_seed(SEED)  # This sets all random seeds in keras\n",
    "tf.config.experimental.enable_op_determinism()  # For complete reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prot_t5_data(pos_file, neg_file):\n",
    "    \"\"\"Load ProtT5 embeddings and align with existing data\"\"\"\n",
    "    # Read positive and negative files\n",
    "    pos_data = []\n",
    "    with open(pos_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(',')\n",
    "            entry = parts[0]\n",
    "            pos = int(parts[1])\n",
    "            embeddings = [float(x) for x in parts[2:]]\n",
    "            pos_data.append((entry, pos, embeddings))\n",
    "            \n",
    "    neg_data = []\n",
    "    with open(neg_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(',')\n",
    "            entry = parts[0]\n",
    "            pos = int(parts[1])\n",
    "            embeddings = [float(x) for x in parts[2:]]\n",
    "            neg_data.append((entry, pos, embeddings))\n",
    "    \n",
    "    # Convert to dictionaries for easy lookup\n",
    "    pos_dict = {(entry, pos): emb for entry, pos, emb in pos_data}\n",
    "    neg_dict = {(entry, pos): emb for entry, pos, emb in neg_data}\n",
    "    \n",
    "    return pos_dict, neg_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_aligned_data(seq_struct_df, pos_dict, neg_dict):\n",
    "    \"\"\"Align ProtT5 embeddings with sequence+structure data\"\"\"\n",
    "    embeddings = []\n",
    "    aligned_indices = []\n",
    "    \n",
    "    for idx, row in seq_struct_df.iterrows():\n",
    "        key = (row['entry'], row['pos'])\n",
    "        emb = pos_dict.get(key) if row['label'] == 1 else neg_dict.get(key)\n",
    "        \n",
    "        if emb is not None:\n",
    "            embeddings.append(emb)\n",
    "            aligned_indices.append(idx)\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    X_prot_t5 = np.array(embeddings)\n",
    "    \n",
    "    # Get aligned sequence+structure data\n",
    "    aligned_df = seq_struct_df.iloc[aligned_indices]\n",
    "    \n",
    "    return X_prot_t5, aligned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence_data(df):\n",
    "    \"\"\"Convert sequences to integer encoding\"\"\"\n",
    "    alphabet = 'ARNDCQEGHILKMFPSTWYV-'\n",
    "    char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "    \n",
    "    sequences = df['sequence'].values\n",
    "    encodings = []\n",
    "    \n",
    "    for seq in sequences:\n",
    "        try:\n",
    "            integer_encoded = [char_to_int[char] for char in seq]\n",
    "            encodings.append(integer_encoded)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sequence: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return np.array(encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_structure_data(df):\n",
    "    \"\"\"Enhanced feature preparation focusing on important features\"\"\"\n",
    "    features_list = []\n",
    "    middle_pos = 16  # Center position\n",
    "    \n",
    "    # Helper function for normalization\n",
    "    def normalize_angles(angle_array):\n",
    "        angle_rad = np.pi * angle_array / 180.0\n",
    "        return np.stack([np.sin(angle_rad), np.cos(angle_rad)], axis=-1)\n",
    "    \n",
    "    # 1. Most important features (importance > 0.04) - using ±0 window\n",
    "    important_features = [\n",
    "        'bfactor',\n",
    "        'distance_to_center',\n",
    "        'sasa',\n",
    "        'omega',\n",
    "        'domain_position'\n",
    "    ]\n",
    "    \n",
    "    for feature in important_features:\n",
    "        feature_arrays = np.array([np.array(eval(x)) for x in df[feature]])\n",
    "        # Take only the center position\n",
    "        center_values = feature_arrays[:, middle_pos]\n",
    "        scaler = RobustScaler()\n",
    "        scaled_values = scaler.fit_transform(center_values.reshape(-1, 1))\n",
    "        features_list.append(scaled_values)\n",
    "    \n",
    "    # 2. Secondary important features (0.03 < importance < 0.04)\n",
    "    secondary_features = [\n",
    "        'chi1', 'chi2', 'chi3', 'chi4',\n",
    "        'curvature', 'psi', 'phi', 'tau',\n",
    "        'packing_density', 'local_hydrophobicity'\n",
    "    ]\n",
    "    \n",
    "    for feature in secondary_features:\n",
    "        feature_arrays = np.array([np.array(eval(x)) for x in df[feature]])\n",
    "        center_values = feature_arrays[:, middle_pos]\n",
    "        if feature in ['phi', 'psi', 'omega', 'tau']:\n",
    "            # Angle features get sin/cos encoding\n",
    "            angle_features = normalize_angles(center_values)\n",
    "            features_list.append(angle_features)\n",
    "        else:\n",
    "            scaler = RobustScaler()\n",
    "            scaled_values = scaler.fit_transform(center_values.reshape(-1, 1))\n",
    "            features_list.append(scaled_values)\n",
    "    \n",
    "    # 3. Special case: hydrophobicity with ±1 window\n",
    "    hydro_arrays = np.array([np.array(eval(x)) for x in df['hydrophobicity']])\n",
    "    hydro_window = hydro_arrays[:, middle_pos-1:middle_pos+2]  # ±1 window\n",
    "    scaler = RobustScaler()\n",
    "    hydro_scaled = scaler.fit_transform(hydro_window.reshape(-1, 1)).reshape(len(hydro_arrays), 3)\n",
    "    features_list.append(hydro_scaled)\n",
    "    \n",
    "    # Combine all features\n",
    "    features = np.concatenate(features_list, axis=-1)\n",
    "    \n",
    "    # Add a dimension for the \"sequence\" length (1 for center position)\n",
    "    features = features.reshape(features.shape[0], 1, -1)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_combined_model_with_prot_t5(seq_length=33, struct_features=None, struct_window=0):\n",
    "    regularizer = tf.keras.regularizers.l2(0.01)\n",
    "    \n",
    "    # Sequence track\n",
    "    seq_input = tf.keras.layers.Input(shape=(seq_length,), name='sequence_input')\n",
    "    x_seq = tf.keras.layers.Embedding(21, 21, input_length=seq_length)(seq_input)\n",
    "    x_seq = tf.keras.layers.Reshape((seq_length, 21, 1))(x_seq)\n",
    "    x_seq = tf.keras.layers.Conv2D(32, kernel_size=(17, 3), activation='relu', padding='valid')(x_seq)\n",
    "    x_seq = tf.keras.layers.BatchNormalization()(x_seq)\n",
    "    x_seq = tf.keras.layers.Dropout(0.4)(x_seq)\n",
    "    x_seq = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x_seq)\n",
    "    x_seq = tf.keras.layers.Flatten()(x_seq)\n",
    "    x_seq = tf.keras.layers.Dense(32, activation='relu', \n",
    "                                 kernel_regularizer=regularizer, \n",
    "                                 name='seq_features')(x_seq)\n",
    "    x_seq = tf.keras.layers.BatchNormalization()(x_seq)\n",
    "    x_seq = tf.keras.layers.Dropout(0.4)(x_seq)\n",
    "\n",
    "    # Structure track\n",
    "    struct_input = tf.keras.layers.Input(shape=(1, struct_features), name='structure_input')\n",
    "    x_struct = struct_input\n",
    "    struct_dense_size = min(struct_features * 2, 128)\n",
    "    \n",
    "    x_struct = tf.keras.layers.Conv1D(32, 3, padding='same', activation='relu')(x_struct)\n",
    "    x_struct = tf.keras.layers.BatchNormalization()(x_struct)\n",
    "    x_struct = tf.keras.layers.Dropout(0.4)(x_struct)\n",
    "    x_struct = tf.keras.layers.Flatten()(x_struct)\n",
    "    x_struct = tf.keras.layers.Dense(struct_dense_size, activation='relu',\n",
    "                                   kernel_regularizer=regularizer)(x_struct)\n",
    "    x_struct = tf.keras.layers.BatchNormalization()(x_struct)\n",
    "    x_struct = tf.keras.layers.Dropout(0.4)(x_struct)\n",
    "\n",
    "    # ProtT5 track\n",
    "    prot_t5_input = tf.keras.layers.Input(shape=(1024,), name='prot_t5_input')\n",
    "    x_prot_t5 = tf.keras.layers.Dense(256, kernel_regularizer=regularizer)(prot_t5_input)\n",
    "    x_prot_t5 = tf.keras.layers.BatchNormalization()(x_prot_t5)\n",
    "    x_prot_t5 = tf.keras.layers.Dropout(0.5)(x_prot_t5)\n",
    "    x_prot_t5 = tf.keras.layers.Dense(128, activation='relu',\n",
    "                                     kernel_regularizer=regularizer)(x_prot_t5)\n",
    "    x_prot_t5 = tf.keras.layers.BatchNormalization()(x_prot_t5)\n",
    "    x_prot_t5 = tf.keras.layers.Dropout(0.5)(x_prot_t5)\n",
    "\n",
    "    # Create learnable weights layer\n",
    "    weight_layer = tf.keras.layers.Dense(3, activation='softmax', name='track_weights')\n",
    "    track_weights = weight_layer(tf.keras.layers.Concatenate()([x_seq, x_struct, x_prot_t5]))\n",
    "\n",
    "    # Apply weights\n",
    "    weighted_seq = tf.keras.layers.Multiply(name='weighted_seq')([\n",
    "        x_seq,\n",
    "        tf.keras.layers.Lambda(lambda x: x[:, 0:1])(track_weights)\n",
    "    ])\n",
    "    \n",
    "    weighted_struct = tf.keras.layers.Multiply(name='weighted_struct')([\n",
    "        x_struct,\n",
    "        tf.keras.layers.Lambda(lambda x: x[:, 1:2])(track_weights)\n",
    "    ])\n",
    "    \n",
    "    weighted_prot_t5 = tf.keras.layers.Multiply(name='weighted_prot_t5')([\n",
    "        x_prot_t5,\n",
    "        tf.keras.layers.Lambda(lambda x: x[:, 2:3])(track_weights)\n",
    "    ])\n",
    "\n",
    "    # Combine features\n",
    "    combined = tf.keras.layers.Concatenate()([weighted_seq, weighted_struct, weighted_prot_t5])\n",
    "\n",
    "    # Final layers with more regularization\n",
    "    x = tf.keras.layers.Dense(64, activation='relu', \n",
    "                            kernel_regularizer=regularizer)(combined)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    x = tf.keras.layers.Dense(32, activation='relu', \n",
    "                            kernel_regularizer=regularizer)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[seq_input, struct_input, prot_t5_input], outputs=outputs)\n",
    "    \n",
    "    def get_track_weights():\n",
    "        w = model.get_layer('track_weights').get_weights()\n",
    "        if len(w) > 0:\n",
    "            weights = w[0]\n",
    "            bias = w[1] if len(w) > 1 else 0\n",
    "            exp_weights = np.exp(np.mean(weights, axis=0) + bias)\n",
    "            normalized = exp_weights / np.sum(exp_weights)\n",
    "            return {\n",
    "                'sequence': float(normalized[0]),\n",
    "                'structure': float(normalized[1]),\n",
    "                'prot_t5': float(normalized[2])\n",
    "            }\n",
    "        return {'sequence': 0.33, 'structure': 0.33, 'prot_t5': 0.34}\n",
    "\n",
    "    model.get_track_weights = get_track_weights\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate():\n",
    "    \"\"\"Training function with track weights\"\"\"\n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    train_df = pd.read_csv(\"../data/processed_features_train_latest.csv\")\n",
    "    test_df = pd.read_csv(\"../data/processed_features_test_latest.csv\")\n",
    "    \n",
    "    # Load ProtT5 embeddings\n",
    "    print(\"Loading ProtT5 embeddings...\")\n",
    "    train_pos_dict, train_neg_dict = load_prot_t5_data(\n",
    "        '/Users/hai/Workspace/UniFreiburg/WS2024/Thesis/LMSuccSite/data/train/features/train_positive_ProtT5-XL-UniRef50.csv',\n",
    "        '/Users/hai/Workspace/UniFreiburg/WS2024/Thesis/LMSuccSite/data/train/features/train_negative_ProtT5-XL-UniRef50.csv'\n",
    "    )\n",
    "    test_pos_dict, test_neg_dict = load_prot_t5_data(\n",
    "        '/Users/hai/Workspace/UniFreiburg/WS2024/Thesis/LMSuccSite/data/test/features/test_positive_ProtT5-XL-UniRef50.csv',\n",
    "        '/Users/hai/Workspace/UniFreiburg/WS2024/Thesis/LMSuccSite/data/test/features/test_negative_ProtT5-XL-UniRef50.csv'\n",
    "    )\n",
    "    \n",
    "    # Align data\n",
    "    print(\"Aligning data...\")\n",
    "    X_train_prot_t5, train_df_aligned = prepare_aligned_data(train_df, train_pos_dict, train_neg_dict)\n",
    "    X_test_prot_t5, test_df_aligned = prepare_aligned_data(test_df, test_pos_dict, test_neg_dict)\n",
    "    \n",
    "    # Prepare other data\n",
    "    X_train_seq = prepare_sequence_data(train_df_aligned)\n",
    "    X_test_seq = prepare_sequence_data(test_df_aligned)\n",
    "    X_train_struct = prepare_structure_data(train_df_aligned)\n",
    "    X_test_struct = prepare_structure_data(test_df_aligned)\n",
    "    \n",
    "    y_train = train_df_aligned['label'].values\n",
    "    y_test = test_df_aligned['label'].values\n",
    "    \n",
    "    # Shuffle training data\n",
    "    shuffle_idx = np.random.RandomState(42).permutation(len(y_train))\n",
    "    X_train_seq = X_train_seq[shuffle_idx]\n",
    "    X_train_struct = X_train_struct[shuffle_idx]\n",
    "    X_train_prot_t5 = X_train_prot_t5[shuffle_idx]\n",
    "    y_train = y_train[shuffle_idx]\n",
    "    \n",
    "    # Shuffle test data\n",
    "    shuffle_idx_test = np.random.RandomState(42).permutation(len(y_test))\n",
    "    X_test_seq = X_test_seq[shuffle_idx_test]\n",
    "    X_test_struct = X_test_struct[shuffle_idx_test]\n",
    "    X_test_prot_t5 = X_test_prot_t5[shuffle_idx_test]\n",
    "    y_test = y_test[shuffle_idx_test]\n",
    "    \n",
    "    # Print class distribution after shuffling\n",
    "    print(\"\\nTraining set distribution (after shuffling):\")\n",
    "    print(pd.Series(y_train).value_counts())\n",
    "    print(\"\\nTest set distribution (after shuffling):\")\n",
    "    print(pd.Series(y_test).value_counts())\n",
    "    \n",
    "    # Print data shapes\n",
    "    print(\"\\nData shapes:\")\n",
    "    print(f\"X_train_seq: {X_train_seq.shape}\")\n",
    "    print(f\"X_train_struct: {X_train_struct.shape}\")\n",
    "    print(f\"X_train_prot_t5: {X_train_prot_t5.shape}\")\n",
    "    print(f\"X_test_seq: {X_test_seq.shape}\")\n",
    "    print(f\"X_test_struct: {X_test_struct.shape}\")\n",
    "    print(f\"X_test_prot_t5: {X_test_prot_t5.shape}\")\n",
    "    print(f\"y_train: {y_train.shape}\")\n",
    "    print(f\"y_test: {y_test.shape}\")\n",
    "    \n",
    "    # Calculate class weights\n",
    "    total_samples = len(y_train)\n",
    "    pos_samples = np.sum(y_train == 1)\n",
    "    neg_samples = np.sum(y_train == 0)\n",
    "    \n",
    "    class_weights = {\n",
    "        0: total_samples / (2 * neg_samples),\n",
    "        1: total_samples / (2 * pos_samples)\n",
    "    }\n",
    "    \n",
    "    # Cross-validation\n",
    "    kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    metrics = {'acc': [], 'balanced_acc': [], 'mcc': [], 'sn': [], 'sp': []}\n",
    "    test_predictions = []\n",
    "    \n",
    "    track_weights_history = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train_seq), 1):\n",
    "        print(f\"\\nFold {fold}/5\")\n",
    "        \n",
    "        # Callbacks\n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=5,\n",
    "                restore_best_weights=True\n",
    "            ),\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.5,\n",
    "                patience=3,\n",
    "                min_lr=1e-6\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Create and compile model\n",
    "        model = create_combined_model_with_prot_t5(\n",
    "            seq_length=33,\n",
    "            struct_features=X_train_struct.shape[2],\n",
    "            struct_window=0\n",
    "        )\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            [X_train_seq[train_idx], X_train_struct[train_idx], X_train_prot_t5[train_idx]], \n",
    "            y_train[train_idx],\n",
    "            validation_data=(\n",
    "                [X_train_seq[val_idx], X_train_struct[val_idx], X_train_prot_t5[val_idx]], \n",
    "                y_train[val_idx]\n",
    "            ),\n",
    "            batch_size=32,\n",
    "            epochs=50,\n",
    "            callbacks=callbacks,\n",
    "            class_weight=class_weights,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Plot training history\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.title(f'Model Accuracy - Fold {fold}')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        y_pred = model.predict([X_train_seq[val_idx], X_train_struct[val_idx], X_train_prot_t5[val_idx]])\n",
    "        y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        cm = confusion_matrix(y_train[val_idx], y_pred_binary)\n",
    "        metrics['acc'].append(accuracy_score(y_train[val_idx], y_pred_binary))\n",
    "        metrics['balanced_acc'].append(balanced_accuracy_score(y_train[val_idx], y_pred_binary))\n",
    "        metrics['mcc'].append(matthews_corrcoef(y_train[val_idx], y_pred_binary))\n",
    "        metrics['sn'].append(cm[1][1]/(cm[1][1]+cm[1][0]))  # Sensitivity\n",
    "        metrics['sp'].append(cm[0][0]/(cm[0][0]+cm[0][1]))  # Specificity\n",
    "        \n",
    "        # Predict on test set\n",
    "        test_pred = model.predict([X_test_seq, X_test_struct, X_test_prot_t5])\n",
    "        test_predictions.append(test_pred)\n",
    "        \n",
    "        # Print fold results\n",
    "        print(f\"\\nFold {fold} Results:\")\n",
    "        print(f\"Accuracy: {metrics['acc'][-1]:.4f}\")\n",
    "        print(f\"Balanced Accuracy: {metrics['balanced_acc'][-1]:.4f}\")\n",
    "        print(f\"MCC: {metrics['mcc'][-1]:.4f}\")\n",
    "        print(f\"Sensitivity: {metrics['sn'][-1]:.4f}\")\n",
    "        print(f\"Specificity: {metrics['sp'][-1]:.4f}\")\n",
    "        \n",
    "        # After training, get and store the learned weights\n",
    "        final_weights = model.get_track_weights()\n",
    "        track_weights_history.append(final_weights)\n",
    "        print(f\"\\nLearned track weights for fold {fold}:\")\n",
    "        print(f\"Sequence weight: {final_weights['sequence']:.4f}\")\n",
    "        print(f\"Structure weight: {final_weights['structure']:.4f}\")\n",
    "        print(f\"ProtT5 weight: {final_weights['prot_t5']:.4f}\")\n",
    "    \n",
    "    # Calculate average weights\n",
    "    avg_seq_weight = np.mean([w['sequence'] for w in track_weights_history])\n",
    "    avg_struct_weight = np.mean([w['structure'] for w in track_weights_history])\n",
    "    avg_prot_t5_weight = np.mean([w['prot_t5'] for w in track_weights_history])\n",
    "    std_seq_weight = np.std([w['sequence'] for w in track_weights_history])\n",
    "    std_struct_weight = np.std([w['structure'] for w in track_weights_history])\n",
    "    std_prot_t5_weight = np.std([w['prot_t5'] for w in track_weights_history])\n",
    "    \n",
    "    print(\"\\nAverage track weights across folds:\")\n",
    "    print(f\"Sequence weight: {avg_seq_weight:.4f} ± {std_seq_weight:.4f}\")\n",
    "    print(f\"Structure weight: {avg_struct_weight:.4f} ± {std_struct_weight:.4f}\")\n",
    "    print(f\"ProtT5 weight: {avg_prot_t5_weight:.4f} ± {std_prot_t5_weight:.4f}\")\n",
    "    \n",
    "    # Print average cross-validation results\n",
    "    print(\"\\nAverage Cross-validation Results:\")\n",
    "    for metric in metrics:\n",
    "        print(f\"{metric.upper()}: {np.mean(metrics[metric]):.4f} ± {np.std(metrics[metric]):.4f}\")\n",
    "    \n",
    "    # Ensemble predictions on test set\n",
    "    test_pred_avg = np.mean(test_predictions, axis=0)\n",
    "    test_pred_binary = (test_pred_avg > 0.5).astype(int)\n",
    "    \n",
    "    # Calculate final test metrics\n",
    "    cm_test = confusion_matrix(y_test, test_pred_binary)\n",
    "    \n",
    "    print(\"\\nFinal Test Set Results:\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, test_pred_binary):.4f}\")\n",
    "    print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, test_pred_binary):.4f}\")\n",
    "    print(f\"MCC: {matthews_corrcoef(y_test, test_pred_binary):.4f}\")\n",
    "    print(f\"Sensitivity: {cm_test[1][1]/(cm_test[1][1]+cm_test[1][0]):.4f}\")\n",
    "    print(f\"Specificity: {cm_test[0][0]/(cm_test[0][0]+cm_test[0][1]):.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm_test)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loading ProtT5 embeddings...\n",
      "Aligning data...\n",
      "\n",
      "Training set distribution (after shuffling):\n",
      "1    4592\n",
      "0    4261\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test set distribution (after shuffling):\n",
      "0    2497\n",
      "1     240\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Data shapes:\n",
      "X_train_seq: (8853, 33)\n",
      "X_train_struct: (8853, 1, 21)\n",
      "X_train_prot_t5: (8853, 1024)\n",
      "X_test_seq: (2737, 33)\n",
      "X_test_struct: (2737, 1, 21)\n",
      "X_test_prot_t5: (2737, 1024)\n",
      "y_train: (8853,)\n",
      "y_test: (2737,)\n",
      "\n",
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hai/miniconda3/envs/bioinf/lib/python3.9/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "2024-12-12 14:23:08.827743: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2024-12-12 14:23:08.827780: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2024-12-12 14:23:08.827792: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2024-12-12 14:23:08.827808: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-12-12 14:23:08.827821: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-12 14:23:10.347135: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 133ms/step - accuracy: 0.5198 - loss: 8.2354 - val_accuracy: 0.5663 - val_loss: 5.6955 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 141ms/step - accuracy: 0.5424 - loss: 5.1448 - val_accuracy: 0.6030 - val_loss: 3.4636 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 136ms/step - accuracy: 0.6043 - loss: 3.1147 - val_accuracy: 0.6352 - val_loss: 2.1939 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 143ms/step - accuracy: 0.6358 - loss: 2.0035 - val_accuracy: 0.6742 - val_loss: 1.5149 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 161ms/step - accuracy: 0.6721 - loss: 1.4331 - val_accuracy: 0.7002 - val_loss: 1.1647 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 151ms/step - accuracy: 0.6911 - loss: 1.1159 - val_accuracy: 0.6985 - val_loss: 1.0094 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 147ms/step - accuracy: 0.7020 - loss: 0.9912 - val_accuracy: 0.7019 - val_loss: 0.9170 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m188/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m4s\u001b[0m 126ms/step - accuracy: 0.7064 - loss: 0.9258"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model_struct, model_full = train_and_evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioinf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
