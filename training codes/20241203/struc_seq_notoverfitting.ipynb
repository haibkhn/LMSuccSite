{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will implement reduce overfitting, not done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, matthews_corrcoef, accuracy_score, balanced_accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "tf.keras.utils.set_random_seed(SEED)  # This sets all random seeds in keras\n",
    "tf.config.experimental.enable_op_determinism()  # For complete reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence_data(df):\n",
    "    \"\"\"Convert sequences to integer encoding\"\"\"\n",
    "    alphabet = 'ARNDCQEGHILKMFPSTWYV-'\n",
    "    char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "    \n",
    "    sequences = df['sequence'].values\n",
    "    encodings = []\n",
    "    \n",
    "    for seq in sequences:\n",
    "        try:\n",
    "            integer_encoded = [char_to_int[char] for char in seq]\n",
    "            encodings.append(integer_encoded)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sequence: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return np.array(encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_structure_data(df):\n",
    "    \"\"\"Enhanced feature preparation with better normalization\"\"\"\n",
    "    \n",
    "    # Normalize angles to their circular nature\n",
    "    def normalize_angles(angle_array):\n",
    "        angle_rad = np.pi * angle_array / 180.0\n",
    "        return np.stack([np.sin(angle_rad), np.cos(angle_rad)], axis=-1)\n",
    "    \n",
    "    # Process each feature type\n",
    "    features_list = []\n",
    "    \n",
    "    # 1. Process angles (phi, psi, omega, tau)\n",
    "    angles = ['phi', 'psi', 'omega', 'tau']\n",
    "    for angle in angles:\n",
    "        # Convert string to array\n",
    "        angle_arrays = np.array([np.array(eval(x)) for x in df[angle]])\n",
    "        # Get sin/cos representations\n",
    "        angle_features = normalize_angles(angle_arrays)\n",
    "        features_list.append(angle_features)\n",
    "    \n",
    "    # 2. Process SASA\n",
    "    sasa_arrays = np.array([np.array(eval(x)) for x in df['sasa']])\n",
    "    scaler = RobustScaler()\n",
    "    sasa_flat = sasa_arrays.reshape(-1, 1)\n",
    "    sasa_scaled = scaler.fit_transform(sasa_flat).reshape(sasa_arrays.shape)\n",
    "    features_list.append(sasa_scaled[..., np.newaxis])\n",
    "    \n",
    "    # 3. Process secondary structure\n",
    "    ss_arrays = np.array([list(seq) for seq in df['ss']])\n",
    "    ss_encoded = np.zeros((len(ss_arrays), ss_arrays.shape[1], 3))\n",
    "    ss_map = {'H': 0, 'E': 1, 'L': 2}\n",
    "    for i in range(len(ss_arrays)):\n",
    "        for j in range(len(ss_arrays[i])):\n",
    "            ss_encoded[i, j, ss_map[ss_arrays[i, j]]] = 1\n",
    "    features_list.append(ss_encoded)\n",
    "    \n",
    "    # Combine all features\n",
    "    features = np.concatenate(features_list, axis=-1)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_combined_model(seq_length=33, struct_features=12, struct_window=0):\n",
    "    \"\"\"Create simpler model with both sequence and structure tracks\"\"\"\n",
    "    middle_pos = seq_length // 2\n",
    "    \n",
    "    # Simplified Sequence track\n",
    "    seq_input = tf.keras.layers.Input(shape=(seq_length,), name='sequence_input')\n",
    "    x_seq = tf.keras.layers.Embedding(21, 21, input_length=seq_length)(seq_input)\n",
    "    x_seq = tf.keras.layers.Conv1D(32, kernel_size=3, activation='relu')(x_seq)\n",
    "    x_seq = tf.keras.layers.GlobalMaxPooling1D()(x_seq)\n",
    "    x_seq = tf.keras.layers.Dense(32, activation='relu')(x_seq)\n",
    "    x_seq = tf.keras.layers.Dropout(0.3)(x_seq)\n",
    "    \n",
    "    # Simplified Structure track\n",
    "    struct_input = tf.keras.layers.Input(shape=(seq_length, struct_features), name='structure_input')\n",
    "    \n",
    "    # Extract middle position(s)\n",
    "    if struct_window == 0:\n",
    "        x_struct = tf.keras.layers.Lambda(\n",
    "            lambda x: x[:, middle_pos:middle_pos+1, :]\n",
    "        )(struct_input)\n",
    "    else:\n",
    "        x_struct = tf.keras.layers.Lambda(\n",
    "            lambda x: x[:, middle_pos-struct_window:middle_pos+struct_window+1, :]\n",
    "        )(struct_input)\n",
    "    \n",
    "    x_struct = tf.keras.layers.Conv1D(32, 3, padding='same', activation='relu')(x_struct)\n",
    "    x_struct = tf.keras.layers.GlobalMaxPooling1D()(x_struct)\n",
    "    x_struct = tf.keras.layers.Dense(32, activation='relu')(x_struct)\n",
    "    x_struct = tf.keras.layers.Dropout(0.3)(x_struct)\n",
    "    \n",
    "    # Simple weight learning\n",
    "    track_weights = tf.keras.layers.Dense(2, activation='softmax', name='track_weights')(\n",
    "        tf.keras.layers.Concatenate()([x_seq, x_struct])\n",
    "    )\n",
    "    \n",
    "    # Apply weights\n",
    "    weighted_seq = tf.keras.layers.Multiply()([\n",
    "        x_seq,\n",
    "        tf.keras.layers.Lambda(lambda x: x[:, 0:1])(track_weights)\n",
    "    ])\n",
    "    \n",
    "    weighted_struct = tf.keras.layers.Multiply()([\n",
    "        x_struct,\n",
    "        tf.keras.layers.Lambda(lambda x: x[:, 1:2])(track_weights)\n",
    "    ])\n",
    "    \n",
    "    # Simple combination\n",
    "    combined = tf.keras.layers.Concatenate()([weighted_seq, weighted_struct])\n",
    "    \n",
    "    # Single dense layer for final prediction\n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(combined)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[seq_input, struct_input], outputs=outputs)\n",
    "    \n",
    "    def get_track_weights():\n",
    "        weights = model.get_layer('track_weights').get_weights()\n",
    "        if len(weights) > 0:\n",
    "            w = weights[0]\n",
    "            b = weights[1] if len(weights) > 1 else 0\n",
    "            exp_weights = np.exp(np.mean(w, axis=0) + b)\n",
    "            normalized = exp_weights / np.sum(exp_weights)\n",
    "            return {\n",
    "                'sequence': float(normalized[0]),\n",
    "                'structure': float(normalized[1])\n",
    "            }\n",
    "        return {'sequence': 0.5, 'structure': 0.5}\n",
    "    \n",
    "    model.get_track_weights = get_track_weights\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(struct_window=0):\n",
    "    \"\"\"Training function with improved regularization and training strategy\"\"\"\n",
    "    # Load and prepare data (keep existing code)\n",
    "    print(\"Loading data...\")\n",
    "    train_df = pd.read_csv(\"../data/processed_data_train_after.csv\")\n",
    "    test_df = pd.read_csv(\"../data/processed_data_test_after.csv\")\n",
    "    \n",
    "    # Prepare data (keep existing data preparation code)\n",
    "    X_train_seq = prepare_sequence_data(train_df)\n",
    "    X_test_seq = prepare_sequence_data(test_df)\n",
    "    X_train_struct = prepare_structure_data(train_df)\n",
    "    X_test_struct = prepare_structure_data(test_df)\n",
    "    \n",
    "    y_train = train_df['label'].values\n",
    "    y_test = test_df['label'].values\n",
    "    \n",
    "    # Calculate class weights (keep existing code)\n",
    "    total_samples = len(y_train)\n",
    "    pos_samples = np.sum(y_train == 1)\n",
    "    neg_samples = np.sum(y_train == 0)\n",
    "    \n",
    "    # class_weights = {\n",
    "    #     0: total_samples / (2 * neg_samples),\n",
    "    #     1: total_samples / (2 * pos_samples)\n",
    "    # }\n",
    "    \n",
    "    # Enhanced callbacks\n",
    "    callbacks = [\n",
    "        # Early stopping with more patience but stricter monitoring\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            mode='min'\n",
    "        ),\n",
    "        # More gradual learning rate reduction\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.2,  # More gentle reduction\n",
    "            patience=5,\n",
    "            min_lr=1e-6,\n",
    "            mode='min',\n",
    "            verbose=1\n",
    "        ),\n",
    "        # Model checkpoint to save best model\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath='best_model_weights.keras',  # Changed from .h5 to .keras\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            mode='min',\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Also modify class weights calculation for better balance\n",
    "    class_weights = {\n",
    "        0: 1,  # Start with equal weights\n",
    "        1: pos_samples / neg_samples  # Adjust weight for minority class\n",
    "    }\n",
    "    \n",
    "    # Cross validation setup\n",
    "    kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    metrics = {'acc': [], 'balanced_acc': [], 'mcc': [], 'sn': [], 'sp': []}\n",
    "    test_predictions = []\n",
    "    track_weights_history = []\n",
    "    \n",
    "    # Print initial shapes\n",
    "    print(\"\\nInitial data shapes:\")\n",
    "    print(f\"X_train_seq shape: {X_train_seq.shape}\")\n",
    "    print(f\"X_train_struct shape: {X_train_struct.shape}\")\n",
    "    print(f\"y_train shape: {y_train.shape}\")\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train_seq), 1):\n",
    "        print(f\"\\nFold {fold}/5\")\n",
    "        \n",
    "        # Print fold data shapes\n",
    "        print(\"\\nFold data shapes:\")\n",
    "        print(f\"Train seq shape: {X_train_seq[train_idx].shape}\")\n",
    "        print(f\"Train struct shape: {X_train_struct[train_idx].shape}\")\n",
    "        print(f\"Train labels shape: {y_train[train_idx].shape}\")\n",
    "        print(f\"Val seq shape: {X_train_seq[val_idx].shape}\")\n",
    "        print(f\"Val struct shape: {X_train_struct[val_idx].shape}\")\n",
    "        print(f\"Val labels shape: {y_train[val_idx].shape}\")\n",
    "        \n",
    "        # Create and compile model\n",
    "        model = create_combined_model(\n",
    "            seq_length=33,\n",
    "            struct_features=X_train_struct.shape[2],\n",
    "            struct_window=struct_window\n",
    "        )\n",
    "        \n",
    "        # Verify input shapes match model expectations\n",
    "        print(\"\\nModel input shapes:\")\n",
    "        print(f\"Sequence input shape: {model.get_layer('sequence_input').input_shape}\")\n",
    "        print(f\"Structure input shape: {model.get_layer('structure_input').input_shape}\")\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=5e-4),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        # Ensure data types are correct\n",
    "        X_train_seq = X_train_seq.astype('float32')\n",
    "        X_train_struct = X_train_struct.astype('float32')\n",
    "        y_train = y_train.astype('float32')\n",
    "        \n",
    "        # Train with validation\n",
    "        try:\n",
    "            history = model.fit(\n",
    "                [X_train_seq[train_idx], X_train_struct[train_idx]],\n",
    "                y_train[train_idx],\n",
    "                validation_data=(\n",
    "                    [X_train_seq[val_idx], X_train_struct[val_idx]],\n",
    "                    y_train[val_idx]\n",
    "                ),\n",
    "                batch_size=32,\n",
    "                epochs=50,\n",
    "                callbacks=callbacks,\n",
    "                class_weight=class_weights,\n",
    "                verbose=1\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError during training: {str(e)}\")\n",
    "            print(\"\\nData information:\")\n",
    "            print(f\"Training data types: {X_train_seq.dtype}, {X_train_struct.dtype}\")\n",
    "            print(f\"Label data type: {y_train.dtype}\")\n",
    "            raise e\n",
    "        \n",
    "        # Plot training curves\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        # Loss subplot\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history.history['loss'], label='Training Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.title(f'Model Loss - Fold {fold}')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Accuracy subplot\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.title(f'Model Accuracy - Fold {fold}')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        y_pred = model.predict([X_train_seq[val_idx], X_train_struct[val_idx]])\n",
    "        y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        cm = confusion_matrix(y_train[val_idx], y_pred_binary)\n",
    "        metrics['acc'].append(accuracy_score(y_train[val_idx], y_pred_binary))\n",
    "        metrics['balanced_acc'].append(balanced_accuracy_score(y_train[val_idx], y_pred_binary))\n",
    "        metrics['mcc'].append(matthews_corrcoef(y_train[val_idx], y_pred_binary))\n",
    "        metrics['sn'].append(cm[1][1]/(cm[1][1]+cm[1][0]))\n",
    "        metrics['sp'].append(cm[0][0]/(cm[0][0]+cm[0][1]))\n",
    "        \n",
    "        # Store track weights\n",
    "        final_weights = model.get_track_weights()\n",
    "        track_weights_history.append(final_weights)\n",
    "        \n",
    "        # Predict on test set\n",
    "        test_pred = model.predict([X_test_seq, X_test_struct])\n",
    "        test_predictions.append(test_pred)\n",
    "        \n",
    "        # Print fold results\n",
    "        print(f\"\\nFold {fold} Results:\")\n",
    "        print(f\"Accuracy: {metrics['acc'][-1]:.4f}\")\n",
    "        print(f\"Balanced Accuracy: {metrics['balanced_acc'][-1]:.4f}\")\n",
    "        print(f\"MCC: {metrics['mcc'][-1]:.4f}\")\n",
    "        print(f\"Sensitivity: {metrics['sn'][-1]:.4f}\")\n",
    "        print(f\"Specificity: {metrics['sp'][-1]:.4f}\")\n",
    "        print(f\"Track weights: Sequence={final_weights['sequence']:.4f}, Structure={final_weights['structure']:.4f}\")\n",
    "    \n",
    "    # Print final results\n",
    "    print(\"\\nFinal Cross-validation Results:\")\n",
    "    for metric in metrics:\n",
    "        print(f\"{metric.upper()}: {np.mean(metrics[metric]):.4f} ± {np.std(metrics[metric]):.4f}\")\n",
    "    \n",
    "    # Calculate and print average track weights\n",
    "    avg_seq_weight = np.mean([w['sequence'] for w in track_weights_history])\n",
    "    avg_struct_weight = np.mean([w['structure'] for w in track_weights_history])\n",
    "    print(f\"\\nAverage track weights:\")\n",
    "    print(f\"Sequence: {avg_seq_weight:.4f} ± {np.std([w['sequence'] for w in track_weights_history]):.4f}\")\n",
    "    print(f\"Structure: {avg_struct_weight:.4f} ± {np.std([w['structure'] for w in track_weights_history]):.4f}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "\n",
      "Initial data shapes:\n",
      "X_train_seq shape: (8850, 33)\n",
      "X_train_struct shape: (8850, 33, 12)\n",
      "y_train shape: (8850,)\n",
      "\n",
      "Fold 1/5\n",
      "\n",
      "Fold data shapes:\n",
      "Train seq shape: (7080, 33)\n",
      "Train struct shape: (7080, 33, 12)\n",
      "Train labels shape: (7080,)\n",
      "Val seq shape: (1770, 33)\n",
      "Val struct shape: (1770, 33, 12)\n",
      "Val labels shape: (1770,)\n",
      "\n",
      "Model input shapes:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hai/miniconda3/envs/bioinf/lib/python3.9/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'InputLayer' object has no attribute 'input_shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 94\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(struct_window)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# Verify input shapes match model expectations\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mModel input shapes:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 94\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSequence input shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39mget_layer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msequence_input\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39minput_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStructure input shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39mget_layer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstructure_input\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39minput_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     97\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m     98\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-4\u001b[39m),\n\u001b[1;32m     99\u001b[0m     loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    100\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    101\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'InputLayer' object has no attribute 'input_shape'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model = train_and_evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Sequence data shape: (8850, 33)\n",
      "Sequence data shape: (2737, 33)\n",
      "Structure data shape: (8850, 33, 12)\n",
      "Structure data shape: (2737, 33, 12)\n",
      "\n",
      "Fold 1/5\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hai/miniconda3/envs/bioinf/lib/python3.9/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.5315 - loss: 0.6655\n",
      "Epoch 1: val_loss improved from inf to 0.68917, saving model to best_model.keras\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Unsupported integer size (0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 299\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 299\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstruct_window\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 225\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(struct_window)\u001b[0m\n\u001b[1;32m    213\u001b[0m model \u001b[38;5;241m=\u001b[39m create_combined_model(\n\u001b[1;32m    214\u001b[0m     seq_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m33\u001b[39m,\n\u001b[1;32m    215\u001b[0m     struct_features\u001b[38;5;241m=\u001b[39mX_train_struct\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m],\n\u001b[1;32m    216\u001b[0m     struct_window\u001b[38;5;241m=\u001b[39mstruct_window\n\u001b[1;32m    217\u001b[0m )\n\u001b[1;32m    219\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m    220\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-4\u001b[39m),\n\u001b[1;32m    221\u001b[0m     loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    222\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    223\u001b[0m )\n\u001b[0;32m--> 225\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_train_seq\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_struct\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_train_seq\u001b[49m\u001b[43m[\u001b[49m\u001b[43mval_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_struct\u001b[49m\u001b[43m[\u001b[49m\u001b[43mval_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mval_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m    237\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;66;03m# Evaluate fold\u001b[39;00m\n\u001b[1;32m    240\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict([X_train_seq[val_idx], X_train_struct[val_idx]])\n",
      "File \u001b[0;32m~/miniconda3/envs/bioinf/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/bioinf/lib/python3.9/site-packages/h5py/_hl/group.py:483\u001b[0m, in \u001b[0;36mGroup.__setitem__\u001b[0;34m(self, name, obj)\u001b[0m\n\u001b[1;32m    480\u001b[0m     htype\u001b[38;5;241m.\u001b[39mcommit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid, name, lcpl\u001b[38;5;241m=\u001b[39mlcpl)\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 483\u001b[0m     ds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_dataset(\u001b[38;5;28;01mNone\u001b[39;00m, data\u001b[38;5;241m=\u001b[39mobj)\n\u001b[1;32m    484\u001b[0m     h5o\u001b[38;5;241m.\u001b[39mlink(ds\u001b[38;5;241m.\u001b[39mid, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid, name, lcpl\u001b[38;5;241m=\u001b[39mlcpl)\n",
      "File \u001b[0;32m~/miniconda3/envs/bioinf/lib/python3.9/site-packages/h5py/_hl/group.py:183\u001b[0m, in \u001b[0;36mGroup.create_dataset\u001b[0;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[1;32m    180\u001b[0m         parent_path, name \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    181\u001b[0m         group \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequire_group(parent_path)\n\u001b[0;32m--> 183\u001b[0m dsid \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmake_new_dset(group, shape, dtype, data, name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    184\u001b[0m dset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mDataset(dsid)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dset\n",
      "File \u001b[0;32m~/miniconda3/envs/bioinf/lib/python3.9/site-packages/h5py/_hl/dataset.py:86\u001b[0m, in \u001b[0;36mmake_new_dset\u001b[0;34m(parent, shape, dtype, data, name, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times, external, track_order, dcpl, dapl, efile_prefix, virtual_prefix, allow_unknown_filter, rdcc_nslots, rdcc_nbytes, rdcc_w0)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     85\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mdtype(dtype)\n\u001b[0;32m---> 86\u001b[0m     tid \u001b[38;5;241m=\u001b[39m \u001b[43mh5t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpy_create\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Legacy\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m((compression, shuffle, fletcher32, maxshape, scaleoffset)) \u001b[38;5;129;01mand\u001b[39;00m chunks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[0;32mh5py/h5t.pyx:1663\u001b[0m, in \u001b[0;36mh5py.h5t.py_create\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5t.pyx:1687\u001b[0m, in \u001b[0;36mh5py.h5t.py_create\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5t.pyx:1705\u001b[0m, in \u001b[0;36mh5py.h5t.py_create\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5t.pyx:1459\u001b[0m, in \u001b[0;36mh5py.h5t._c_int\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Unsupported integer size (0)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, matthews_corrcoef, accuracy_score, balanced_accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "def prepare_sequence_data(df):\n",
    "    \"\"\"Convert sequences to integer encoding with validation\"\"\"\n",
    "    alphabet = 'ARNDCQEGHILKMFPSTWYV-'\n",
    "    char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "    \n",
    "    sequences = df['sequence'].values\n",
    "    encodings = []\n",
    "    \n",
    "    for i, seq in enumerate(sequences):\n",
    "        try:\n",
    "            if pd.isna(seq):\n",
    "                print(f\"Warning: Found null sequence at index {i}\")\n",
    "                continue\n",
    "            if not isinstance(seq, str):\n",
    "                print(f\"Warning: Non-string sequence at index {i}: {type(seq)}\")\n",
    "                continue\n",
    "            integer_encoded = [char_to_int[char] for char in seq]\n",
    "            encodings.append(integer_encoded)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sequence at index {i}: {str(e)}\")\n",
    "            print(f\"Sequence: {seq}\")\n",
    "            continue\n",
    "    \n",
    "    encodings = np.array(encodings, dtype=np.float32)\n",
    "    print(f\"Sequence data shape: {encodings.shape}\")\n",
    "    return encodings\n",
    "\n",
    "def prepare_structure_data(df):\n",
    "    \"\"\"Enhanced feature preparation with validation\"\"\"\n",
    "    def normalize_angles(angle_array):\n",
    "        angle_rad = np.pi * angle_array / 180.0\n",
    "        return np.stack([np.sin(angle_rad), np.cos(angle_rad)], axis=-1)\n",
    "    \n",
    "    features_list = []\n",
    "    \n",
    "    # Process angles\n",
    "    for angle in ['phi', 'psi', 'omega', 'tau']:\n",
    "        try:\n",
    "            angle_arrays = np.array([np.array(eval(x)) for x in df[angle]])\n",
    "            angle_features = normalize_angles(angle_arrays)\n",
    "            features_list.append(angle_features)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {angle}: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    # Process SASA\n",
    "    try:\n",
    "        sasa_arrays = np.array([np.array(eval(x)) for x in df['sasa']])\n",
    "        scaler = RobustScaler()\n",
    "        sasa_flat = sasa_arrays.reshape(-1, 1)\n",
    "        sasa_scaled = scaler.fit_transform(sasa_flat).reshape(sasa_arrays.shape)\n",
    "        features_list.append(sasa_scaled[..., np.newaxis])\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing SASA: {str(e)}\")\n",
    "        raise\n",
    "    \n",
    "    # Process secondary structure\n",
    "    try:\n",
    "        ss_arrays = np.array([list(seq) for seq in df['ss']])\n",
    "        ss_encoded = np.zeros((len(ss_arrays), ss_arrays.shape[1], 3))\n",
    "        ss_map = {'H': 0, 'E': 1, 'L': 2}\n",
    "        for i in range(len(ss_arrays)):\n",
    "            for j in range(len(ss_arrays[i])):\n",
    "                ss_encoded[i, j, ss_map[ss_arrays[i, j]]] = 1\n",
    "        features_list.append(ss_encoded)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing secondary structure: {str(e)}\")\n",
    "        raise\n",
    "    \n",
    "    # Combine features\n",
    "    features = np.concatenate(features_list, axis=-1)\n",
    "    features = features.astype(np.float32)\n",
    "    print(f\"Structure data shape: {features.shape}\")\n",
    "    return features\n",
    "\n",
    "def create_combined_model(seq_length=33, struct_features=12, struct_window=0):\n",
    "    \"\"\"Create simplified model with both sequence and structure tracks\"\"\"\n",
    "    middle_pos = seq_length // 2\n",
    "    \n",
    "    # Sequence track\n",
    "    seq_input = tf.keras.layers.Input(shape=(seq_length,), name='sequence_input')\n",
    "    x_seq = tf.keras.layers.Embedding(21, 21, input_length=seq_length)(seq_input)\n",
    "    x_seq = tf.keras.layers.Conv1D(32, kernel_size=3, activation='relu')(x_seq)\n",
    "    x_seq = tf.keras.layers.GlobalMaxPooling1D()(x_seq)\n",
    "    x_seq = tf.keras.layers.Dense(32, activation='relu')(x_seq)\n",
    "    x_seq = tf.keras.layers.Dropout(0.3)(x_seq)\n",
    "    \n",
    "    # Structure track\n",
    "    struct_input = tf.keras.layers.Input(shape=(seq_length, struct_features), name='structure_input')\n",
    "    \n",
    "    if struct_window == 0:\n",
    "        x_struct = tf.keras.layers.Lambda(\n",
    "            lambda x: x[:, middle_pos:middle_pos+1, :]\n",
    "        )(struct_input)\n",
    "    else:\n",
    "        x_struct = tf.keras.layers.Lambda(\n",
    "            lambda x: x[:, middle_pos-struct_window:middle_pos+struct_window+1, :]\n",
    "        )(struct_input)\n",
    "    \n",
    "    x_struct = tf.keras.layers.Conv1D(32, 3, padding='same', activation='relu')(x_struct)\n",
    "    x_struct = tf.keras.layers.GlobalMaxPooling1D()(x_struct)\n",
    "    x_struct = tf.keras.layers.Dense(32, activation='relu')(x_struct)\n",
    "    x_struct = tf.keras.layers.Dropout(0.3)(x_struct)\n",
    "    \n",
    "    # Track weights\n",
    "    track_weights = tf.keras.layers.Dense(\n",
    "        2, activation='softmax', name='track_weights'\n",
    "    )(tf.keras.layers.Concatenate()([x_seq, x_struct]))\n",
    "    \n",
    "    # Apply weights\n",
    "    weighted_seq = tf.keras.layers.Multiply()(\n",
    "        [x_seq, tf.keras.layers.Lambda(lambda x: x[:, 0:1])(track_weights)]\n",
    "    )\n",
    "    weighted_struct = tf.keras.layers.Multiply()(\n",
    "        [x_struct, tf.keras.layers.Lambda(lambda x: x[:, 1:2])(track_weights)]\n",
    "    )\n",
    "    \n",
    "    # Combine and predict\n",
    "    combined = tf.keras.layers.Concatenate()([weighted_seq, weighted_struct])\n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(combined)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[seq_input, struct_input], outputs=outputs)\n",
    "    \n",
    "    def get_track_weights():\n",
    "        weights = model.get_layer('track_weights').get_weights()\n",
    "        if len(weights) > 0:\n",
    "            w = weights[0]\n",
    "            b = weights[1] if len(weights) > 1 else 0\n",
    "            exp_weights = np.exp(np.mean(w, axis=0) + b)\n",
    "            normalized = exp_weights / np.sum(exp_weights)\n",
    "            return {\n",
    "                'sequence': float(normalized[0]),\n",
    "                'structure': float(normalized[1])\n",
    "            }\n",
    "        return {'sequence': 0.5, 'structure': 0.5}\n",
    "    \n",
    "    model.get_track_weights = get_track_weights\n",
    "    return model\n",
    "\n",
    "def train_and_evaluate(struct_window=0):\n",
    "    \"\"\"Training function with improved monitoring and validation\"\"\"\n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    train_df = pd.read_csv(\"../data/processed_data_train_after.csv\")\n",
    "    test_df = pd.read_csv(\"../data/processed_data_test_after.csv\")\n",
    "    \n",
    "    # Prepare data\n",
    "    X_train_seq = prepare_sequence_data(train_df)\n",
    "    X_test_seq = prepare_sequence_data(test_df)\n",
    "    X_train_struct = prepare_structure_data(train_df)\n",
    "    X_test_struct = prepare_structure_data(test_df)\n",
    "    \n",
    "    y_train = train_df['label'].values.astype(np.float32)\n",
    "    y_test = test_df['label'].values.astype(np.float32)\n",
    "    \n",
    "    # Calculate class weights\n",
    "    total = len(y_train)\n",
    "    pos = np.sum(y_train == 1)\n",
    "    neg = np.sum(y_train == 0)\n",
    "    class_weights = {0: 1.0, 1: neg/pos}\n",
    "    \n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            mode='min'\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.2,\n",
    "            patience=5,\n",
    "            min_lr=1e-6,\n",
    "            mode='min',\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath='best_model.keras',\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            mode='min',\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Cross validation\n",
    "    kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    metrics = {'acc': [], 'balanced_acc': [], 'mcc': [], 'sn': [], 'sp': []}\n",
    "    test_predictions = []\n",
    "    track_weights_history = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train_seq), 1):\n",
    "        print(f\"\\nFold {fold}/5\")\n",
    "        \n",
    "        model = create_combined_model(\n",
    "            seq_length=33,\n",
    "            struct_features=X_train_struct.shape[2],\n",
    "            struct_window=struct_window\n",
    "        )\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=5e-4),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        history = model.fit(\n",
    "            [X_train_seq[train_idx], X_train_struct[train_idx]],\n",
    "            y_train[train_idx],\n",
    "            validation_data=(\n",
    "                [X_train_seq[val_idx], X_train_struct[val_idx]],\n",
    "                y_train[val_idx]\n",
    "            ),\n",
    "            batch_size=32,\n",
    "            epochs=50,\n",
    "            callbacks=callbacks,\n",
    "            class_weight=class_weights,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Evaluate fold\n",
    "        y_pred = model.predict([X_train_seq[val_idx], X_train_struct[val_idx]])\n",
    "        y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "        \n",
    "        cm = confusion_matrix(y_train[val_idx], y_pred_binary)\n",
    "        metrics['acc'].append(accuracy_score(y_train[val_idx], y_pred_binary))\n",
    "        metrics['balanced_acc'].append(balanced_accuracy_score(y_train[val_idx], y_pred_binary))\n",
    "        metrics['mcc'].append(matthews_corrcoef(y_train[val_idx], y_pred_binary))\n",
    "        metrics['sn'].append(cm[1][1]/(cm[1][1]+cm[1][0]))\n",
    "        metrics['sp'].append(cm[0][0]/(cm[0][0]+cm[0][1]))\n",
    "        \n",
    "        # Store weights and predictions\n",
    "        track_weights_history.append(model.get_track_weights())\n",
    "        test_pred = model.predict([X_test_seq, X_test_struct])\n",
    "        test_predictions.append(test_pred)\n",
    "        \n",
    "        # Print fold results\n",
    "        print(f\"\\nFold {fold} Results:\")\n",
    "        for metric in metrics:\n",
    "            print(f\"{metric.upper()}: {metrics[metric][-1]:.4f}\")\n",
    "        print(f\"Track weights: {track_weights_history[-1]}\")\n",
    "        \n",
    "        # Plot training curves\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history.history['loss'], label='Training Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.title(f'Loss - Fold {fold}')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.title(f'Accuracy - Fold {fold}')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    # Print final results\n",
    "    print(\"\\nFinal Cross-validation Results:\")\n",
    "    for metric in metrics:\n",
    "        mean_val = np.mean(metrics[metric])\n",
    "        std_val = np.std(metrics[metric])\n",
    "        print(f\"{metric.upper()}: {mean_val:.4f} ± {std_val:.4f}\")\n",
    "    \n",
    "    # Calculate ensemble predictions\n",
    "    test_pred_avg = np.mean(test_predictions, axis=0)\n",
    "    test_pred_binary = (test_pred_avg > 0.5).astype(int)\n",
    "    \n",
    "    # Final metrics\n",
    "    final_cm = confusion_matrix(y_test, test_pred_binary)\n",
    "    print(\"\\nFinal Test Set Results:\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, test_pred_binary):.4f}\")\n",
    "    print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, test_pred_binary):.4f}\")\n",
    "    print(f\"MCC: {matthews_corrcoef(y_test, test_pred_binary):.4f}\")\n",
    "    print(f\"Sensitivity: {final_cm[1][1]/(final_cm[1][1]+final_cm[1][0]):.4f}\")\n",
    "    print(f\"Specificity: {final_cm[0][0]/(final_cm[0][0]+final_cm[0][1]):.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = train_and_evaluate(struct_window=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioinf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
