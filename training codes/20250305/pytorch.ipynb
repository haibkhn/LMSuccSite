{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Preparing sequence data...\n",
      "Preparing GNN data...\n",
      "Processing samples for GNN...\n",
      "Processed 0/8853 samples\n",
      "Processed 1000/8853 samples\n",
      "Processed 2000/8853 samples\n",
      "Processed 3000/8853 samples\n",
      "Processed 4000/8853 samples\n",
      "Processed 5000/8853 samples\n",
      "Processed 6000/8853 samples\n",
      "Processed 7000/8853 samples\n",
      "Processed 8000/8853 samples\n",
      "Created 8853 graph data objects\n",
      "Processing samples for GNN...\n",
      "Processed 0/2737 samples\n",
      "Processed 1000/2737 samples\n",
      "Processed 2000/2737 samples\n",
      "Created 2737 graph data objects\n",
      "\n",
      "Class distribution:\n",
      "Train: [4261 4592]\n",
      "Test: [2497  240]\n",
      "\n",
      "Fold 1/5\n",
      "Epoch 5/50, Train Loss: 0.5680, Val Loss: 32.5805, Val Acc: 0.6742\n",
      "Epoch 10/50, Train Loss: 0.5109, Val Loss: 32.4111, Val Acc: 0.6759\n",
      "Epoch 15/50, Train Loss: 0.4582, Val Loss: 30.6606, Val Acc: 0.6934\n",
      "Epoch 20/50, Train Loss: 0.4441, Val Loss: 29.7007, Val Acc: 0.7030\n",
      "Epoch 25/50, Train Loss: 0.4134, Val Loss: 29.6443, Val Acc: 0.7036\n",
      "Epoch 30/50, Train Loss: 0.3968, Val Loss: 27.2163, Val Acc: 0.7278\n",
      "Epoch 35/50, Train Loss: 0.3804, Val Loss: 28.1762, Val Acc: 0.7182\n",
      "Early stopping after 37 epochs\n",
      "\n",
      "Fold 1 Results:\n",
      "Accuracy: 0.7069\n",
      "Balanced Accuracy: 0.7039\n",
      "MCC: 0.4140\n",
      "Sensitivity: 0.7845\n",
      "Specificity: 0.6232\n",
      "Confusion Matrix:\n",
      "[[531 321]\n",
      " [198 721]]\n",
      "\n",
      "Fold 2/5\n",
      "Epoch 5/50, Train Loss: 0.5795, Val Loss: 36.7024, Val Acc: 0.6330\n",
      "Epoch 10/50, Train Loss: 0.5334, Val Loss: 36.4201, Val Acc: 0.6358\n",
      "Epoch 15/50, Train Loss: 0.4883, Val Loss: 31.6770, Val Acc: 0.6832\n",
      "Epoch 20/50, Train Loss: 0.4603, Val Loss: 29.9266, Val Acc: 0.7007\n",
      "Epoch 25/50, Train Loss: 0.3997, Val Loss: 31.0559, Val Acc: 0.6894\n",
      "Early stopping after 26 epochs\n",
      "\n",
      "Fold 2 Results:\n",
      "Accuracy: 0.6990\n",
      "Balanced Accuracy: 0.7017\n",
      "MCC: 0.4064\n",
      "Sensitivity: 0.6311\n",
      "Specificity: 0.7723\n",
      "Confusion Matrix:\n",
      "[[658 194]\n",
      " [339 580]]\n",
      "\n",
      "Fold 3/5\n",
      "Epoch 5/50, Train Loss: 0.5697, Val Loss: 34.4438, Val Acc: 0.6556\n",
      "Epoch 10/50, Train Loss: 0.5248, Val Loss: 30.6606, Val Acc: 0.6934\n",
      "Epoch 15/50, Train Loss: 0.4774, Val Loss: 30.0395, Val Acc: 0.6996\n",
      "Epoch 20/50, Train Loss: 0.4339, Val Loss: 29.9266, Val Acc: 0.7007\n",
      "Epoch 25/50, Train Loss: 0.3933, Val Loss: 27.9503, Val Acc: 0.7205\n",
      "Epoch 30/50, Train Loss: 0.3587, Val Loss: 27.7244, Val Acc: 0.7228\n",
      "Epoch 35/50, Train Loss: 0.3342, Val Loss: 26.7645, Val Acc: 0.7324\n",
      "Epoch 40/50, Train Loss: 0.3103, Val Loss: 27.2727, Val Acc: 0.7273\n",
      "Epoch 45/50, Train Loss: 0.3018, Val Loss: 26.1434, Val Acc: 0.7386\n",
      "Epoch 50/50, Train Loss: 0.2828, Val Loss: 26.0305, Val Acc: 0.7397\n",
      "\n",
      "Fold 3 Results:\n",
      "Accuracy: 0.7397\n",
      "Balanced Accuracy: 0.7365\n",
      "MCC: 0.4815\n",
      "Sensitivity: 0.8224\n",
      "Specificity: 0.6506\n",
      "Confusion Matrix:\n",
      "[[555 298]\n",
      " [163 755]]\n",
      "\n",
      "Fold 4/5\n",
      "Epoch 5/50, Train Loss: 0.5638, Val Loss: 34.9718, Val Acc: 0.6503\n",
      "Epoch 10/50, Train Loss: 0.5202, Val Loss: 32.0904, Val Acc: 0.6791\n",
      "Epoch 15/50, Train Loss: 0.4779, Val Loss: 27.9096, Val Acc: 0.7209\n",
      "Epoch 20/50, Train Loss: 0.4387, Val Loss: 28.4181, Val Acc: 0.7158\n",
      "Epoch 25/50, Train Loss: 0.4052, Val Loss: 28.8701, Val Acc: 0.7113\n",
      "Early stopping after 28 epochs\n",
      "\n",
      "Fold 4 Results:\n",
      "Accuracy: 0.7034\n",
      "Balanced Accuracy: 0.6981\n",
      "MCC: 0.4146\n",
      "Sensitivity: 0.8388\n",
      "Specificity: 0.5575\n",
      "Confusion Matrix:\n",
      "[[475 377]\n",
      " [148 770]]\n",
      "\n",
      "Fold 5/5\n",
      "Epoch 5/50, Train Loss: 0.5749, Val Loss: 31.7514, Val Acc: 0.6825\n",
      "Epoch 10/50, Train Loss: 0.5142, Val Loss: 29.3785, Val Acc: 0.7062\n",
      "Epoch 15/50, Train Loss: 0.4628, Val Loss: 27.5706, Val Acc: 0.7243\n",
      "Epoch 20/50, Train Loss: 0.4172, Val Loss: 30.0565, Val Acc: 0.6994\n",
      "Epoch 25/50, Train Loss: 0.3792, Val Loss: 26.9492, Val Acc: 0.7305\n",
      "Epoch 30/50, Train Loss: 0.3427, Val Loss: 30.0000, Val Acc: 0.7000\n",
      "Early stopping after 35 epochs\n",
      "\n",
      "Fold 5 Results:\n",
      "Accuracy: 0.7198\n",
      "Balanced Accuracy: 0.7194\n",
      "MCC: 0.4388\n",
      "Sensitivity: 0.7298\n",
      "Specificity: 0.7089\n",
      "Confusion Matrix:\n",
      "[[604 248]\n",
      " [248 670]]\n",
      "\n",
      "Average Cross-validation Results for GCN:\n",
      "accuracy: 0.7138 ± 0.0147\n",
      "balanced_accuracy: 0.7119 ± 0.0143\n",
      "mcc: 0.4310 ± 0.0275\n",
      "sensitivity: 0.7613 ± 0.0751\n",
      "specificity: 0.6625 ± 0.0734\n",
      "\n",
      "Final Test Set Results for GCN:\n",
      "Accuracy: 0.6712\n",
      "Balanced Accuracy: 0.7313\n",
      "MCC: 0.2692\n",
      "Sensitivity: 0.8042\n",
      "Specificity: 0.6584\n",
      "Confusion Matrix:\n",
      "[[1644  853]\n",
      " [  47  193]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv, MessagePassing, global_add_pool\n",
    "from torch_geometric.data import Data, Batch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, matthews_corrcoef, accuracy_score, balanced_accuracy_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "class GNNModel(nn.Module):\n",
    "    def __init__(self, seq_length=33, node_features=23, edge_features=18, hidden_dim=32, gnn_type='gcn'):\n",
    "        super(GNNModel, self).__init__()\n",
    "        \n",
    "        # Sequence track (CNN)\n",
    "        self.embedding = nn.Embedding(21, 21)\n",
    "        self.conv = nn.Conv2d(1, 32, kernel_size=(17, 3), padding=0)\n",
    "        self.bn = nn.BatchNorm2d(32)\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # Calculate flattened size after conv + pool\n",
    "        # For the CNN part, we need to calculate the output dimensions\n",
    "        conv_output_height = seq_length - 17 + 1  # Valid padding\n",
    "        conv_output_width = 21 - 3 + 1  # Valid padding\n",
    "        pool_output_height = conv_output_height // 2\n",
    "        pool_output_width = conv_output_width // 2\n",
    "        flatten_size = 32 * pool_output_height * pool_output_width\n",
    "        \n",
    "        self.seq_fc = nn.Linear(flatten_size, hidden_dim)\n",
    "        \n",
    "        # GNN track\n",
    "        if gnn_type == 'gcn':\n",
    "            self.gnn_layer = GCNConv(node_features, hidden_dim)\n",
    "        elif gnn_type == 'gat':\n",
    "            self.gnn_layer = GATConv(node_features, hidden_dim // 4, heads=4)\n",
    "        \n",
    "        # Final classifier\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, 64)\n",
    "        self.bn_fc = nn.BatchNorm1d(64)\n",
    "        self.dropout_fc = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "        \n",
    "        # Store GNN type\n",
    "        self.gnn_type = gnn_type\n",
    "    \n",
    "    def forward(self, seq_data, graph_data_batch):\n",
    "        # Process sequence data\n",
    "        x_seq = self.embedding(seq_data)  # [batch_size, seq_length, 21]\n",
    "        x_seq = x_seq.reshape(x_seq.size(0), 1, x_seq.size(1), x_seq.size(2))  # [batch_size, 1, seq_length, 21]\n",
    "        x_seq = self.conv(x_seq)\n",
    "        x_seq = self.bn(x_seq)\n",
    "        x_seq = F.relu(x_seq)\n",
    "        x_seq = self.dropout(x_seq)\n",
    "        x_seq = self.pool(x_seq)\n",
    "        x_seq = x_seq.flatten(1)  # Flatten all dimensions except batch\n",
    "        x_seq = self.seq_fc(x_seq)\n",
    "        x_seq = F.relu(x_seq)\n",
    "        \n",
    "        # Process graph data\n",
    "        x = graph_data_batch.x\n",
    "        edge_index = graph_data_batch.edge_index\n",
    "        edge_attr = graph_data_batch.edge_attr\n",
    "        node_mask = graph_data_batch.node_mask\n",
    "        batch = graph_data_batch.batch\n",
    "        \n",
    "        if self.gnn_type == 'gcn':\n",
    "            # Standard GCNConv doesn't use edge features directly\n",
    "            x_gnn = self.gnn_layer(x, edge_index)\n",
    "            # Apply node mask\n",
    "            x_gnn = x_gnn * node_mask.unsqueeze(-1)\n",
    "            x_gnn = F.relu(x_gnn)\n",
    "        elif self.gnn_type == 'gat':\n",
    "            # Standard GATConv doesn't use edge features directly\n",
    "            x_gnn = self.gnn_layer(x, edge_index)\n",
    "            # Apply node mask\n",
    "            x_gnn = x_gnn * node_mask.unsqueeze(-1)\n",
    "            x_gnn = F.relu(x_gnn)\n",
    "        \n",
    "        # Global pooling over nodes\n",
    "        x_gnn = global_add_pool(x_gnn, batch)\n",
    "        \n",
    "        # Combine tracks\n",
    "        combined = torch.cat([x_seq, x_gnn], dim=1)\n",
    "        \n",
    "        # Final classification\n",
    "        x = self.fc1(combined)\n",
    "        x = self.bn_fc(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout_fc(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "def create_edge_features(i, j, dist_map, sequence, ss_string, sasa_vals, K_POS=16):\n",
    "    \"\"\"\n",
    "    Create edge features with proper padding handling using sequence\n",
    "    Returns a fixed-size numpy array\n",
    "    \"\"\"\n",
    "    edge_features = np.zeros(18, dtype=np.float32)  # Pre-allocate fixed size array\n",
    "    distance = dist_map[i,j]\n",
    "    \n",
    "    # Check if either position is padded using sequence\n",
    "    is_i_padded = sequence[i] == '-'\n",
    "    is_j_padded = sequence[j] == '-'\n",
    "    \n",
    "    if is_i_padded or is_j_padded:\n",
    "        return edge_features  # Return zero array for padded positions\n",
    "    \n",
    "    feature_idx = 0\n",
    "    \n",
    "    # 1. Distance Features\n",
    "    # Distance bins (4 features)\n",
    "    if distance <= 4.0:\n",
    "        edge_features[0] = 1.0\n",
    "    elif distance <= 8.0:\n",
    "        edge_features[1] = 1.0\n",
    "    elif distance <= 12.0:\n",
    "        edge_features[2] = 1.0\n",
    "    else:\n",
    "        edge_features[3] = 1.0\n",
    "    feature_idx += 4\n",
    "    \n",
    "    # Continuous distance feature (1 feature)\n",
    "    edge_features[feature_idx] = 1/distance\n",
    "    feature_idx += 1\n",
    "    \n",
    "    # 2. Sequential Features (2 features)\n",
    "    seq_dist = abs(i - j)\n",
    "    edge_features[feature_idx] = float(seq_dist == 1)  # Is sequential\n",
    "    edge_features[feature_idx + 1] = seq_dist / 32.0  # Normalized distance\n",
    "    feature_idx += 2\n",
    "    \n",
    "    # 3. K-relative Features (2 features)\n",
    "    edge_features[feature_idx] = float(i == K_POS or j == K_POS)  # Is K-connected\n",
    "    edge_features[feature_idx + 1] = min(abs(i - K_POS), abs(j - K_POS)) / 16.0  # Min distance to K\n",
    "    feature_idx += 2\n",
    "    \n",
    "    # 4. Secondary Structure Interaction (6 features)\n",
    "    ss_pairs = ['HH', 'HE', 'HL', 'EE', 'EL', 'LL']\n",
    "    ss_pair = ''.join(sorted([ss_string[i], ss_string[j]]))\n",
    "    for idx, pair in enumerate(ss_pairs):\n",
    "        edge_features[feature_idx + idx] = float(ss_pair == pair)\n",
    "    feature_idx += 6\n",
    "    \n",
    "    # 5. SASA Interaction (2 features)\n",
    "    edge_features[feature_idx] = abs(sasa_vals[i] - sasa_vals[j])  # SASA difference\n",
    "    edge_features[feature_idx + 1] = (sasa_vals[i] + sasa_vals[j]) / 2  # SASA average\n",
    "    \n",
    "    return edge_features\n",
    "\n",
    "def prepare_gnn_data_pyg(df, threshold=8.0):\n",
    "    \"\"\"\n",
    "    Prepare node and edge features using sequence for padding detection\n",
    "    Returns a list of PyG Data objects\n",
    "    \"\"\"\n",
    "    graph_data_list = []\n",
    "    K_POS = 16\n",
    "    \n",
    "    print(\"Processing samples for GNN...\")\n",
    "    for idx, row in df.iterrows():\n",
    "        sequence = row['sequence']\n",
    "        # Use sequence for padding detection\n",
    "        is_padded = [pos for pos, aa in enumerate(sequence) if aa == '-']\n",
    "        \n",
    "        # Pre-allocate node features array\n",
    "        sample_nodes = np.zeros((33, 23), dtype=np.float32)\n",
    "        \n",
    "        for pos in range(33):\n",
    "            if pos in is_padded:\n",
    "                continue  # Keep zeros for padded positions\n",
    "            \n",
    "            feature_idx = 0\n",
    "            \n",
    "            # Backbone angles\n",
    "            for angle in ['phi', 'psi', 'omega']:\n",
    "                angle_vals = np.array(eval(row[angle]))\n",
    "                angle_rad = np.pi * angle_vals[pos] / 180.0\n",
    "                sample_nodes[pos, feature_idx:feature_idx+2] = [np.sin(angle_rad), np.cos(angle_rad)]\n",
    "                feature_idx += 2\n",
    "            \n",
    "            # SASA\n",
    "            sasa_vals = np.array(eval(row['sasa']))\n",
    "            sample_nodes[pos, feature_idx] = sasa_vals[pos]\n",
    "            feature_idx += 1\n",
    "            \n",
    "            # SS\n",
    "            ss_val = row['ss'][pos]\n",
    "            ss_onehot = [1 if ss_val == ss_type else 0 for ss_type in 'HEL']\n",
    "            sample_nodes[pos, feature_idx:feature_idx+3] = ss_onehot\n",
    "            feature_idx += 3\n",
    "            \n",
    "            # plDDT\n",
    "            plddt_vals = np.array(eval(row['plDDT']))\n",
    "            sample_nodes[pos, feature_idx] = plddt_vals[pos]\n",
    "            feature_idx += 1\n",
    "            \n",
    "            # Chi angles\n",
    "            for chi in ['chi1', 'chi2', 'chi3', 'chi4']:\n",
    "                chi_vals = np.array(eval(row[chi]))\n",
    "                chi_rad = np.pi * chi_vals[pos] / 180.0\n",
    "                sample_nodes[pos, feature_idx:feature_idx+2] = [np.sin(chi_rad), np.cos(chi_rad)]\n",
    "                feature_idx += 2\n",
    "            \n",
    "            # K-relative features\n",
    "            sample_nodes[pos, feature_idx] = abs(pos - K_POS) / 16.0  # Distance to K\n",
    "            feature_idx += 1\n",
    "            sample_nodes[pos, feature_idx] = float(pos == K_POS)  # Is K position\n",
    "            feature_idx += 1\n",
    "            sample_nodes[pos, feature_idx] = float(sequence[pos] == 'K')  # Is K amino acid\n",
    "        \n",
    "        # Create edges\n",
    "        dist_map = np.array(eval(row['distance_map'])).reshape(33, 33)\n",
    "        edges = []\n",
    "        edge_attrs = []\n",
    "        \n",
    "        # Pre-calculate SASA values\n",
    "        sasa_vals = np.array(eval(row['sasa']))\n",
    "        \n",
    "        for i in range(33):\n",
    "            if i in is_padded:\n",
    "                continue\n",
    "                \n",
    "            for j in range(33):\n",
    "                if j in is_padded or i == j:\n",
    "                    continue\n",
    "                    \n",
    "                if dist_map[i,j] != -1 and dist_map[i,j] < threshold:\n",
    "                    edges.append([i, j])\n",
    "                    edge_attrs.append(\n",
    "                        create_edge_features(\n",
    "                            i, j, dist_map, sequence, row['ss'], \n",
    "                            sasa_vals, K_POS\n",
    "                        )\n",
    "                    )\n",
    "        \n",
    "        # Create node mask\n",
    "        node_mask = np.array([0.0 if pos in is_padded else 1.0 for pos in range(33)], dtype=np.float32)\n",
    "        \n",
    "        # Convert to PyG Data object\n",
    "        if len(edges) == 0:\n",
    "            # Handle case with no edges\n",
    "            edge_index = torch.zeros((2, 0), dtype=torch.long)\n",
    "            edge_attr = torch.zeros((0, 18), dtype=torch.float)\n",
    "        else:\n",
    "            # PyG expects edge_index to be of shape [2, num_edges]\n",
    "            edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "            edge_attr = torch.tensor(edge_attrs, dtype=torch.float)\n",
    "        \n",
    "        x = torch.tensor(sample_nodes, dtype=torch.float)\n",
    "        node_mask = torch.tensor(node_mask, dtype=torch.float)\n",
    "        \n",
    "        # Create PyG Data object\n",
    "        data = Data(\n",
    "            x=x,\n",
    "            edge_index=edge_index,\n",
    "            edge_attr=edge_attr,\n",
    "            node_mask=node_mask,\n",
    "            y=torch.tensor([row['label']], dtype=torch.float)\n",
    "        )\n",
    "        \n",
    "        graph_data_list.append(data)\n",
    "        \n",
    "        if idx % 1000 == 0:\n",
    "            print(f\"Processed {idx}/{len(df)} samples\")\n",
    "    \n",
    "    print(f\"Created {len(graph_data_list)} graph data objects\")\n",
    "    \n",
    "    return graph_data_list\n",
    "\n",
    "def prepare_sequence_data_pyg(df):\n",
    "    \"\"\"Convert sequences to integer encoding for PyTorch\"\"\"\n",
    "    alphabet = 'ARNDCQEGHILKMFPSTWYV-'\n",
    "    char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "    \n",
    "    sequences = df['sequence'].values\n",
    "    encodings = []\n",
    "    \n",
    "    for seq in sequences:\n",
    "        try:\n",
    "            integer_encoded = [char_to_int[char] for char in seq]\n",
    "            encodings.append(integer_encoded)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sequence: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return torch.tensor(encodings, dtype=torch.long)\n",
    "\n",
    "class SequenceGraphDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, sequence_data, graph_data_list):\n",
    "        self.sequence_data = sequence_data\n",
    "        self.graph_data_list = graph_data_list\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequence_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequence_data[idx], self.graph_data_list[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    seq_data, graph_data = zip(*batch)\n",
    "    seq_data = torch.stack(seq_data)\n",
    "    graph_data = Batch.from_data_list(graph_data)\n",
    "    return seq_data, graph_data\n",
    "\n",
    "def train_epoch(model, data_loader, optimizer, device, class_weights=None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for seq_data, graph_data in data_loader:\n",
    "        seq_data = seq_data.to(device)\n",
    "        graph_data = graph_data.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(seq_data, graph_data)\n",
    "        target = graph_data.y.view(-1, 1).float()\n",
    "        \n",
    "        if class_weights is not None:\n",
    "            # Apply class weights to loss calculation\n",
    "            weight = torch.ones_like(target)\n",
    "            weight[target == 0] = class_weights[0]\n",
    "            weight[target == 1] = class_weights[1]\n",
    "            loss = F.binary_cross_entropy(output, target, weight=weight)\n",
    "        else:\n",
    "            loss = F.binary_cross_entropy(output, target)\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def eval_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for seq_data, graph_data in data_loader:\n",
    "            seq_data = seq_data.to(device)\n",
    "            graph_data = graph_data.to(device)\n",
    "            \n",
    "            output = model(seq_data, graph_data)\n",
    "            pred = (output > 0.5).float().cpu().numpy()\n",
    "            target = graph_data.y.view(-1, 1).cpu().numpy()\n",
    "            \n",
    "            predictions.extend(pred)\n",
    "            targets.extend(target)\n",
    "    \n",
    "    predictions = np.array(predictions).flatten()\n",
    "    targets = np.array(targets).flatten()\n",
    "    \n",
    "    return predictions, targets\n",
    "\n",
    "def print_metrics(y_true, y_pred):\n",
    "    \"\"\"Print comprehensive evaluation metrics\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    balanced_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "    \n",
    "    if cm.shape[0] > 1 and cm.shape[1] > 1:  # Ensure there are both positive and negative classes\n",
    "        sensitivity = cm[1][1]/(cm[1][1]+cm[1][0]) if (cm[1][1]+cm[1][0]) > 0 else 0\n",
    "        specificity = cm[0][0]/(cm[0][0]+cm[0][1]) if (cm[0][0]+cm[0][1]) > 0 else 0\n",
    "    else:\n",
    "        sensitivity = specificity = 0\n",
    "    \n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Balanced Accuracy: {balanced_acc:.4f}\")\n",
    "    print(f\"MCC: {mcc:.4f}\")\n",
    "    print(f\"Sensitivity: {sensitivity:.4f}\")\n",
    "    print(f\"Specificity: {specificity:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'balanced_accuracy': balanced_acc,\n",
    "        'mcc': mcc,\n",
    "        'sensitivity': sensitivity,\n",
    "        'specificity': specificity\n",
    "    }\n",
    "\n",
    "def train_with_cv(train_df, test_df, threshold=8.0, gnn_type='gcn', epochs=50, batch_size=32, lr=1e-3, n_folds=5):\n",
    "    \"\"\"\n",
    "    Training function with cross-validation for PyTorch Geometric implementation\n",
    "    Args:\n",
    "        train_df: training dataframe\n",
    "        test_df: test dataframe\n",
    "        threshold: distance threshold for edge creation\n",
    "        gnn_type: 'simple', 'gcn', or 'gat'\n",
    "        epochs: maximum number of training epochs\n",
    "        batch_size: batch size for training\n",
    "        lr: learning rate\n",
    "        n_folds: number of cross-validation folds\n",
    "    \"\"\"\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Prepare sequence data\n",
    "    print(\"Preparing sequence data...\")\n",
    "    train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    X_train_seq = prepare_sequence_data_pyg(train_df)\n",
    "    X_test_seq = prepare_sequence_data_pyg(test_df)\n",
    "    \n",
    "    # Prepare graph data\n",
    "    print(\"Preparing GNN data...\")\n",
    "    train_graph_data = prepare_gnn_data_pyg(train_df, threshold=threshold)\n",
    "    test_graph_data = prepare_gnn_data_pyg(test_df, threshold=threshold)\n",
    "    \n",
    "    y_train = train_df['label'].values\n",
    "    y_test = test_df['label'].values\n",
    "    \n",
    "    # Print class distribution\n",
    "    print(\"\\nClass distribution:\")\n",
    "    print(\"Train:\", np.bincount(y_train))\n",
    "    print(\"Test:\", np.bincount(y_test))\n",
    "    \n",
    "    # Cross-validation\n",
    "    kfold = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    metrics = {\n",
    "        'accuracy': [], 'balanced_accuracy': [], \n",
    "        'mcc': [], 'sensitivity': [], 'specificity': []\n",
    "    }\n",
    "    test_predictions = []\n",
    "    \n",
    "    # Create full test dataset\n",
    "    test_dataset = SequenceGraphDataset(X_test_seq, test_graph_data)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train_seq, y_train), 1):\n",
    "        print(f\"\\nFold {fold}/{n_folds}\")\n",
    "        \n",
    "        # Calculate class weights\n",
    "        total = len(train_idx)\n",
    "        pos = np.sum(y_train[train_idx] == 1)\n",
    "        neg = np.sum(y_train[train_idx] == 0)\n",
    "        class_weights = {\n",
    "            0: total / (2 * neg),\n",
    "            1: total / (2 * pos)\n",
    "        }\n",
    "        class_weights_tensor = (class_weights[0], class_weights[1])\n",
    "        \n",
    "        # Create datasets for this fold\n",
    "        train_fold_seq = X_train_seq[train_idx]\n",
    "        val_fold_seq = X_train_seq[val_idx]\n",
    "        \n",
    "        train_fold_graph = [train_graph_data[i] for i in train_idx]\n",
    "        val_fold_graph = [train_graph_data[i] for i in val_idx]\n",
    "        \n",
    "        train_fold_dataset = SequenceGraphDataset(train_fold_seq, train_fold_graph)\n",
    "        val_fold_dataset = SequenceGraphDataset(val_fold_seq, val_fold_graph)\n",
    "        \n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            train_fold_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=True,\n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "        \n",
    "        val_loader = torch.utils.data.DataLoader(\n",
    "            val_fold_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=False,\n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "        \n",
    "        # Create model\n",
    "        model = GNNModel(\n",
    "            seq_length=33,\n",
    "            node_features=23,\n",
    "            edge_features=18,\n",
    "            hidden_dim=32,\n",
    "            gnn_type=gnn_type\n",
    "        ).to(device)\n",
    "        \n",
    "        # Optimizer\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, \n",
    "            mode='min', \n",
    "            factor=0.5,\n",
    "            patience=4,\n",
    "            min_lr=1e-6\n",
    "        )\n",
    "        \n",
    "        # Early stopping\n",
    "        best_val_loss = float('inf')\n",
    "        early_stop_counter = 0\n",
    "        early_stop_patience = 7\n",
    "        best_model_state = None\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            # Train\n",
    "            train_loss = train_epoch(model, train_loader, optimizer, device, class_weights_tensor)\n",
    "            \n",
    "            # Validate\n",
    "            val_pred, val_true = eval_model(model, val_loader, device)\n",
    "            val_loss = F.binary_cross_entropy(\n",
    "                torch.tensor(val_pred, dtype=torch.float), \n",
    "                torch.tensor(val_true, dtype=torch.float)\n",
    "            ).item()\n",
    "            \n",
    "            val_acc = accuracy_score(val_true, (val_pred > 0.5).astype(int))\n",
    "\n",
    "            \n",
    "            # Update scheduler\n",
    "            scheduler.step(val_loss)\n",
    "            \n",
    "            # Check for early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                early_stop_counter = 0\n",
    "                # Save best model\n",
    "                best_model_state = model.state_dict().copy()\n",
    "            else:\n",
    "                early_stop_counter += 1\n",
    "                \n",
    "            if early_stop_counter >= early_stop_patience:\n",
    "                print(f\"Early stopping after {epoch+1} epochs\")\n",
    "                break\n",
    "                \n",
    "            # Print progress\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        # Load best model\n",
    "        model.load_state_dict(best_model_state)\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        val_pred, val_true = eval_model(model, val_loader, device)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        print(f\"\\nFold {fold} Results:\")\n",
    "        fold_metrics = print_metrics(val_true, val_pred)\n",
    "        \n",
    "        # Store metrics\n",
    "        for key in metrics:\n",
    "            metrics[key].append(fold_metrics[key])\n",
    "        \n",
    "        # Predict on test set\n",
    "        test_pred, test_true = eval_model(model, test_loader, device)\n",
    "        test_predictions.append(test_pred)\n",
    "    \n",
    "    # Print average CV results\n",
    "    print(f\"\\nAverage Cross-validation Results for {gnn_type.upper()}:\")\n",
    "    cv_results = {}\n",
    "    for metric in metrics:\n",
    "        mean = np.mean(metrics[metric])\n",
    "        std = np.std(metrics[metric])\n",
    "        print(f\"{metric}: {mean:.4f} ± {std:.4f}\")\n",
    "        cv_results[metric] = {'mean': mean, 'std': std}\n",
    "    \n",
    "    # Ensemble predictions on test set\n",
    "    test_pred_avg = np.mean(test_predictions, axis=0)\n",
    "    test_pred_binary = (test_pred_avg > 0.5).astype(int)\n",
    "    \n",
    "    # Print final test results\n",
    "    print(f\"\\nFinal Test Set Results for {gnn_type.upper()}:\")\n",
    "    test_results = print_metrics(y_test, test_pred_binary)\n",
    "    \n",
    "    return {\n",
    "        'cv_results': cv_results,\n",
    "        'test_results': test_results,\n",
    "        'model': model\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    train_df = pd.read_csv(\"../data/processed_features_fixed_train_contactmap.csv\")\n",
    "    test_df = pd.read_csv(\"../data/processed_features_fixed_test_contactmap.csv\")\n",
    "    \n",
    "    # Minimal data\n",
    "    # train_df = pd.read_csv(\"../data/processed_features_fixed_train_contactmap_copy.csv\")\n",
    "    # test_df = pd.read_csv(\"../data/processed_features_fixed_test_contactmap_copy.csv\")\n",
    "    \n",
    "    # Train with GCN (basic implementation)\n",
    "    results = train_with_cv(train_df, test_df, threshold=8.0, gnn_type='gcn')\n",
    "    \n",
    "    # # To try all GNN types:\n",
    "    # results = train_with_cv(train_df, test_df, threshold=8.0, gnn_type=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Preparing sequence data...\n",
      "Preparing GNN data...\n",
      "Processing samples for GNN...\n",
      "Processed 0/8853 samples\n",
      "Processed 1000/8853 samples\n",
      "Processed 2000/8853 samples\n",
      "Processed 3000/8853 samples\n",
      "Processed 4000/8853 samples\n",
      "Processed 5000/8853 samples\n",
      "Processed 6000/8853 samples\n",
      "Processed 7000/8853 samples\n",
      "Processed 8000/8853 samples\n",
      "Created 8853 graph data objects\n",
      "Processing samples for GNN...\n",
      "Processed 0/2737 samples\n",
      "Processed 1000/2737 samples\n",
      "Processed 2000/2737 samples\n",
      "Created 2737 graph data objects\n",
      "\n",
      "Class distribution:\n",
      "Train: [4261 4592]\n",
      "Test: [2497  240]\n",
      "\n",
      "Fold 1/5\n",
      "Epoch 5/50, Train Loss: 0.5648, Val Loss: 33.0322, Val Acc: 0.6697\n",
      "Epoch 10/50, Train Loss: 0.5021, Val Loss: 33.4274, Val Acc: 0.6657\n",
      "Epoch 15/50, Train Loss: 0.4657, Val Loss: 27.5551, Val Acc: 0.7244\n",
      "Epoch 20/50, Train Loss: 0.4237, Val Loss: 26.1434, Val Acc: 0.7386\n",
      "Epoch 25/50, Train Loss: 0.3993, Val Loss: 26.2564, Val Acc: 0.7374\n",
      "Early stopping after 30 epochs\n",
      "\n",
      "Fold 1 Results:\n",
      "Accuracy: 0.7357\n",
      "Balanced Accuracy: 0.7315\n",
      "MCC: 0.4768\n",
      "Sensitivity: 0.8433\n",
      "Specificity: 0.6197\n",
      "Confusion Matrix:\n",
      "[[528 324]\n",
      " [144 775]]\n",
      "\n",
      "Fold 2/5\n",
      "Epoch 5/50, Train Loss: 0.5621, Val Loss: 33.4274, Val Acc: 0.6657\n",
      "Epoch 10/50, Train Loss: 0.5159, Val Loss: 33.3710, Val Acc: 0.6663\n",
      "Epoch 15/50, Train Loss: 0.4652, Val Loss: 31.6206, Val Acc: 0.6838\n",
      "Epoch 20/50, Train Loss: 0.4141, Val Loss: 28.8538, Val Acc: 0.7115\n",
      "Epoch 25/50, Train Loss: 0.3821, Val Loss: 30.2654, Val Acc: 0.6973\n",
      "Epoch 30/50, Train Loss: 0.3496, Val Loss: 28.7973, Val Acc: 0.7120\n",
      "Epoch 35/50, Train Loss: 0.3170, Val Loss: 27.9503, Val Acc: 0.7205\n",
      "Epoch 40/50, Train Loss: 0.2984, Val Loss: 28.3456, Val Acc: 0.7165\n",
      "Early stopping after 42 epochs\n",
      "\n",
      "Fold 2 Results:\n",
      "Accuracy: 0.7081\n",
      "Balanced Accuracy: 0.7081\n",
      "MCC: 0.4159\n",
      "Sensitivity: 0.7084\n",
      "Specificity: 0.7077\n",
      "Confusion Matrix:\n",
      "[[603 249]\n",
      " [268 651]]\n",
      "\n",
      "Fold 3/5\n",
      "Epoch 5/50, Train Loss: 0.5702, Val Loss: 32.1287, Val Acc: 0.6787\n",
      "Epoch 10/50, Train Loss: 0.5184, Val Loss: 30.5477, Val Acc: 0.6945\n",
      "Epoch 15/50, Train Loss: 0.4734, Val Loss: 30.5477, Val Acc: 0.6945\n",
      "Epoch 20/50, Train Loss: 0.4429, Val Loss: 27.8374, Val Acc: 0.7216\n",
      "Epoch 25/50, Train Loss: 0.4034, Val Loss: 28.5150, Val Acc: 0.7149\n",
      "Early stopping after 29 epochs\n",
      "\n",
      "Fold 3 Results:\n",
      "Accuracy: 0.7211\n",
      "Balanced Accuracy: 0.7187\n",
      "MCC: 0.4418\n",
      "Sensitivity: 0.7832\n",
      "Specificity: 0.6542\n",
      "Confusion Matrix:\n",
      "[[558 295]\n",
      " [199 719]]\n",
      "\n",
      "Fold 4/5\n",
      "Epoch 5/50, Train Loss: 0.5664, Val Loss: 34.4068, Val Acc: 0.6559\n",
      "Epoch 10/50, Train Loss: 0.5079, Val Loss: 30.5650, Val Acc: 0.6944\n",
      "Epoch 15/50, Train Loss: 0.4559, Val Loss: 28.4181, Val Acc: 0.7158\n",
      "Epoch 20/50, Train Loss: 0.4109, Val Loss: 29.4350, Val Acc: 0.7056\n",
      "Early stopping after 22 epochs\n",
      "\n",
      "Fold 4 Results:\n",
      "Accuracy: 0.7034\n",
      "Balanced Accuracy: 0.7046\n",
      "MCC: 0.4093\n",
      "Sensitivity: 0.6732\n",
      "Specificity: 0.7359\n",
      "Confusion Matrix:\n",
      "[[627 225]\n",
      " [300 618]]\n",
      "\n",
      "Fold 5/5\n",
      "Epoch 5/50, Train Loss: 0.5649, Val Loss: 39.6610, Val Acc: 0.6034\n",
      "Early stopping after 10 epochs\n",
      "\n",
      "Fold 5 Results:\n",
      "Accuracy: 0.6277\n",
      "Balanced Accuracy: 0.6376\n",
      "MCC: 0.3231\n",
      "Sensitivity: 0.3704\n",
      "Specificity: 0.9049\n",
      "Confusion Matrix:\n",
      "[[771  81]\n",
      " [578 340]]\n",
      "\n",
      "Average Cross-validation Results for GAT:\n",
      "accuracy: 0.6992 ± 0.0375\n",
      "balanced_accuracy: 0.7001 ± 0.0326\n",
      "mcc: 0.4134 ± 0.0510\n",
      "sensitivity: 0.6757 ± 0.1637\n",
      "specificity: 0.7245 ± 0.0989\n",
      "\n",
      "Final Test Set Results for GAT:\n",
      "Accuracy: 0.7285\n",
      "Balanced Accuracy: 0.7213\n",
      "MCC: 0.2710\n",
      "Sensitivity: 0.7125\n",
      "Specificity: 0.7301\n",
      "Confusion Matrix:\n",
      "[[1823  674]\n",
      " [  69  171]]\n"
     ]
    }
   ],
   "source": [
    "results = train_with_cv(train_df, test_df, threshold=8.0, gnn_type=\"gat\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lysine-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
