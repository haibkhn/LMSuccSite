{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will use all dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading complete data...\n",
      "Loading structure data first...\n",
      "\n",
      "Processing positive data...\n",
      "Processing negative data...\n",
      "\n",
      "Warning: Found duplicate entries:\n",
      "       entry   pos  label\n",
      "5149  A2A274   574      0\n",
      "9481  A2A274   574      0\n",
      "6617  A2A274   725      0\n",
      "8100  A2A274   725      0\n",
      "5098  A2A7A7   331      0\n",
      "...      ...   ...    ...\n",
      "8637  V4Z729   112      0\n",
      "4957  V4Z7M7   122      0\n",
      "6616  V4Z7M7   122      0\n",
      "7535  V4Z7M7  2271      0\n",
      "7586  V4Z7M7  2271      0\n",
      "\n",
      "[435 rows x 3 columns]\n",
      "\n",
      "Dataset statistics:\n",
      "Total entries: 9500\n",
      "Positive examples: 4750\n",
      "Negative examples: 4750\n",
      "Entries with structure: 8853\n",
      "Unique proteins: 2193\n",
      "\n",
      "Example entries to verify parsing:\n",
      "    entry  pos                           sequence  label  has_structure\n",
      "0  P0A823  227  EILAYKAEISAEGMALKKSLPVTL---------      1           True\n",
      "1  P75863  344  CGSCRVQLLEGEVTPLKKSAMGDDGTILCCSCV      1           True\n",
      "2  P0ABS8    7  ----------MLKNLAKLDQTEMDKVNVDLAAA      1           True\n",
      "3  P0ABS8   74  AHRLASVNLSRLPYEPKLK--------------      1           True\n",
      "4  P0A7D7    3  --------------MQKQAELYRGKAKTVYSTE      1           True\n",
      "Loading structure data first...\n",
      "\n",
      "Processing positive data...\n",
      "Processing negative data...\n",
      "\n",
      "Dataset statistics:\n",
      "Total entries: 3224\n",
      "Positive examples: 253\n",
      "Negative examples: 2971\n",
      "Entries with structure: 2737\n",
      "Unique proteins: 123\n",
      "\n",
      "Example entries to verify parsing:\n",
      "    entry  pos                           sequence  label  has_structure\n",
      "0  P09030   70  VFYHGQKGHYGVALLTKETPIAVRRGFPGDDEE      1           True\n",
      "1  P09030   24  INGLRARPHQLEAIVEKHQPDVIGLQETKVHDD      1           True\n",
      "2  P06996   31  VAGAANAAEVYNKDGNKLDLYGKVDGLHYFSDN      1           True\n",
      "3  P17854  102  TYRFIDELTDKLKLNLKVYRATESAAWQEARYG      1           True\n",
      "4  P17854  136  LWEQGVEGIEKYNDINKVEPMNRALKELNAQTW      1           True\n",
      "Loading ProtT5 embeddings...\n",
      "Aligning data with ProtT5 embeddings...\n",
      "\n",
      "Data Statistics:\n",
      "Total training samples: 9500\n",
      "Training samples with structure: 8853 (93.2%)\n",
      "Total test samples: 3224\n",
      "Test samples with structure: 2737 (84.9%)\n",
      "\n",
      "Training set distribution:\n",
      "1    4750\n",
      "0    4750\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test set distribution:\n",
      "0    2971\n",
      "1     253\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Data shapes:\n",
      "X_train_seq_all: (9500, 33)\n",
      "X_train_prot_t5: (9500, 1024)\n",
      "X_train_struct: (8853, 1, 21)\n",
      "\n",
      "Fold 1/5\n",
      "\n",
      "Training sequence+ProtT5 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hai/miniconda3/envs/bioinf/lib/python3.9/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "2024-12-12 05:14:01.925439: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2024-12-12 05:14:01.925470: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2024-12-12 05:14:01.925479: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2024-12-12 05:14:01.925493: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-12-12 05:14:01.925504: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-12 05:14:02.646776: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 28ms/step - accuracy: 0.6077 - loss: 0.6520 - val_accuracy: 0.6989 - val_loss: 0.5692 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 23ms/step - accuracy: 0.7215 - loss: 0.5599 - val_accuracy: 0.7142 - val_loss: 0.5586 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 23ms/step - accuracy: 0.7437 - loss: 0.5311 - val_accuracy: 0.7195 - val_loss: 0.5589 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - accuracy: 0.7500 - loss: 0.5037 - val_accuracy: 0.7232 - val_loss: 0.5485 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 23ms/step - accuracy: 0.7661 - loss: 0.4804 - val_accuracy: 0.7342 - val_loss: 0.5469 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 23ms/step - accuracy: 0.7837 - loss: 0.4646 - val_accuracy: 0.7547 - val_loss: 0.5375 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 23ms/step - accuracy: 0.8019 - loss: 0.4337 - val_accuracy: 0.7711 - val_loss: 0.5167 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 23ms/step - accuracy: 0.8152 - loss: 0.4003 - val_accuracy: 0.7711 - val_loss: 0.5144 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 23ms/step - accuracy: 0.8389 - loss: 0.3677 - val_accuracy: 0.7732 - val_loss: 0.5149 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 23ms/step - accuracy: 0.8406 - loss: 0.3618 - val_accuracy: 0.7863 - val_loss: 0.5230 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - accuracy: 0.8533 - loss: 0.3440 - val_accuracy: 0.7826 - val_loss: 0.5321 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 23ms/step - accuracy: 0.8630 - loss: 0.3130 - val_accuracy: 0.7895 - val_loss: 0.5497 - learning_rate: 5.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 23ms/step - accuracy: 0.8804 - loss: 0.2901 - val_accuracy: 0.7874 - val_loss: 0.5598 - learning_rate: 5.0000e-04\n",
      "\n",
      "Training full model...\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 8853 is out of bounds for axis 0 with size 8853",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 772\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_with_struct, model_no_struct\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 772\u001b[0m     model_with_struct, model_no_struct \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 642\u001b[0m, in \u001b[0;36mtrain_and_evaluate_split\u001b[0;34m()\u001b[0m\n\u001b[1;32m    629\u001b[0m model_with_struct \u001b[38;5;241m=\u001b[39m create_combined_model_with_prot_t5(\n\u001b[1;32m    630\u001b[0m     seq_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m33\u001b[39m,\n\u001b[1;32m    631\u001b[0m     struct_features\u001b[38;5;241m=\u001b[39mX_train_struct\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m],\n\u001b[1;32m    632\u001b[0m     struct_window\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    633\u001b[0m )\n\u001b[1;32m    635\u001b[0m model_with_struct\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m    636\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m),\n\u001b[1;32m    637\u001b[0m     loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    638\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    639\u001b[0m )\n\u001b[1;32m    641\u001b[0m history_struct \u001b[38;5;241m=\u001b[39m model_with_struct\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m--> 642\u001b[0m     [\u001b[43mX_train_seq_struct\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstruct_train_idx\u001b[49m\u001b[43m]\u001b[49m, X_train_struct[struct_train_idx], X_train_prot_t5_struct[struct_train_idx]],\n\u001b[1;32m    643\u001b[0m     y_train_struct[struct_train_idx],\n\u001b[1;32m    644\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    645\u001b[0m         [X_train_seq_struct[struct_val_idx], X_train_struct[struct_val_idx], X_train_prot_t5_struct[struct_val_idx]],\n\u001b[1;32m    646\u001b[0m         y_train_struct[struct_val_idx]\n\u001b[1;32m    647\u001b[0m     ),\n\u001b[1;32m    648\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m    649\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[1;32m    650\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m    651\u001b[0m     class_weight\u001b[38;5;241m=\u001b[39mclass_weights,\n\u001b[1;32m    652\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    653\u001b[0m )\n\u001b[1;32m    655\u001b[0m \u001b[38;5;66;03m# Store track weights\u001b[39;00m\n\u001b[1;32m    656\u001b[0m track_weights \u001b[38;5;241m=\u001b[39m model_with_struct\u001b[38;5;241m.\u001b[39mget_track_weights()\n",
      "\u001b[0;31mIndexError\u001b[0m: index 8853 is out of bounds for axis 0 with size 8853"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, matthews_corrcoef, accuracy_score, balanced_accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "def load_prot_t5_data(pos_file, neg_file):\n",
    "    \"\"\"Load ProtT5 embeddings and align with existing data\"\"\"\n",
    "    # Read positive and negative files\n",
    "    pos_data = []\n",
    "    with open(pos_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(',')\n",
    "            entry = parts[0]\n",
    "            pos = int(parts[1])\n",
    "            embeddings = [float(x) for x in parts[2:]]\n",
    "            pos_data.append((entry, pos, embeddings))\n",
    "            \n",
    "    neg_data = []\n",
    "    with open(neg_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(',')\n",
    "            entry = parts[0]\n",
    "            pos = int(parts[1])\n",
    "            embeddings = [float(x) for x in parts[2:]]\n",
    "            neg_data.append((entry, pos, embeddings))\n",
    "    \n",
    "    # Convert to dictionaries for easy lookup\n",
    "    pos_dict = {(entry, pos): emb for entry, pos, emb in pos_data}\n",
    "    neg_dict = {(entry, pos): emb for entry, pos, emb in neg_data}\n",
    "    \n",
    "    return pos_dict, neg_dict\n",
    "\n",
    "def prepare_aligned_data(seq_struct_df, pos_dict, neg_dict):\n",
    "    \"\"\"Align ProtT5 embeddings with sequence+structure data\"\"\"\n",
    "    embeddings = []\n",
    "    aligned_indices = []\n",
    "    \n",
    "    for idx, row in seq_struct_df.iterrows():\n",
    "        key = (row['entry'], row['pos'])\n",
    "        emb = pos_dict.get(key) if row['label'] == 1 else neg_dict.get(key)\n",
    "        \n",
    "        if emb is not None:\n",
    "            embeddings.append(emb)\n",
    "            aligned_indices.append(idx)\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    X_prot_t5 = np.array(embeddings)\n",
    "    \n",
    "    # Get aligned sequence+structure data\n",
    "    aligned_df = seq_struct_df.iloc[aligned_indices]\n",
    "    \n",
    "    return X_prot_t5, aligned_df\n",
    "\n",
    "def extract_entry_id(header):\n",
    "    \"\"\"Extract entry ID between first and second '|' characters if present, otherwise return as is\"\"\"\n",
    "    if '|' in header:\n",
    "        try:\n",
    "            return header.split('|')[1]\n",
    "        except:\n",
    "            print(f\"Warning: Could not parse header with pipes: {header}\")\n",
    "            return header\n",
    "    else:\n",
    "        # If no pipes, assume it's already an ID\n",
    "        return header\n",
    "\n",
    "def load_complete_data(mode='train', exclude_emb=False):\n",
    "    \"\"\"\n",
    "    Load data line by line ensuring perfect matching between FASTA and ProtT5 files\n",
    "    \"\"\"\n",
    "    # Set paths based on mode\n",
    "    if mode == 'train':\n",
    "        pos_fasta = '../../data/train/fasta/positive_sites.fasta'\n",
    "        neg_fasta = '../../data/train/fasta/negative_sites.fasta'\n",
    "        pos_prott5_path = '../../data/train/features/train_positive_ProtT5-XL-UniRef50.csv'\n",
    "        neg_prott5_path = '../../data/train/features/train_negative_ProtT5-XL-UniRef50.csv'\n",
    "        struct_path = \"../data/processed_features_train_latest.csv\"\n",
    "    else:  # test\n",
    "        pos_fasta = '../../data/test/fasta/test_positive_sites.fasta'\n",
    "        neg_fasta = '../../data/test/fasta/test_negative_sites.fasta'\n",
    "        pos_prott5_path = '../../data/test/features/test_positive_ProtT5-XL-UniRef50.csv'\n",
    "        neg_prott5_path = '../../data/test/features/test_negative_ProtT5-XL-UniRef50.csv'\n",
    "        struct_path = \"../data/processed_features_test_latest.csv\"\n",
    "\n",
    "    print(\"Loading structure data first...\")\n",
    "    struct_data = pd.read_csv(struct_path)\n",
    "    # Clean structure data entry IDs\n",
    "    struct_data['entry'] = struct_data['entry'].apply(lambda x: extract_entry_id(x) if isinstance(x, str) else x)\n",
    "    \n",
    "    # Create dictionary for quick lookup of structure data\n",
    "    struct_dict = {}\n",
    "    for _, row in struct_data.iterrows():\n",
    "        key = (row['entry'], row['pos'])\n",
    "        struct_dict[key] = row.to_dict()\n",
    "\n",
    "    print(\"\\nProcessing positive data...\")\n",
    "    positive_data = []\n",
    "    \n",
    "    # Process positive data\n",
    "    with open(pos_fasta) as fasta_file:\n",
    "        fasta_lines = fasta_file.readlines()\n",
    "    \n",
    "    # Read ProtT5 positive data\n",
    "    prott5_pos = pd.read_csv(pos_prott5_path, header=None)\n",
    "    \n",
    "    for i in range(0, len(fasta_lines), 2):\n",
    "        # Process FASTA header\n",
    "        header = fasta_lines[i].strip()[1:]  # remove '>'\n",
    "        sequence = fasta_lines[i + 1].strip()\n",
    "        \n",
    "        # Parse header - extract entry ID between | characters and position after |-|\n",
    "        entry = extract_entry_id(header.split('|-|')[0])\n",
    "        pos = int(header.split('|-|')[1])\n",
    "        \n",
    "        # Get corresponding ProtT5 line\n",
    "        prott5_line = prott5_pos.iloc[i//2]\n",
    "        \n",
    "        # Create data entry\n",
    "        data_entry = {\n",
    "            'entry': entry,\n",
    "            'pos': pos,\n",
    "            'sequence': sequence,\n",
    "            'label': 1,\n",
    "            'has_structure': False\n",
    "        }\n",
    "        \n",
    "        # Add ProtT5 embeddings if requested\n",
    "        if not exclude_emb:\n",
    "            for j in range(2, len(prott5_line)):  # skip entry and pos columns\n",
    "                data_entry[f'emb_{j-2}'] = prott5_line[j]\n",
    "        \n",
    "        # Add structure features if available\n",
    "        if (entry, pos) in struct_dict:\n",
    "            data_entry['has_structure'] = True\n",
    "            struct_features = struct_dict[(entry, pos)]\n",
    "            for key, value in struct_features.items():\n",
    "                if key not in ['entry', 'pos', 'sequence', 'label']:\n",
    "                    data_entry[key] = value\n",
    "        \n",
    "        positive_data.append(data_entry)\n",
    "    \n",
    "    print(\"Processing negative data...\")\n",
    "    negative_data = []\n",
    "    \n",
    "    # Process negative data\n",
    "    with open(neg_fasta) as fasta_file:\n",
    "        fasta_lines = fasta_file.readlines()\n",
    "    \n",
    "    # Read ProtT5 negative data\n",
    "    prott5_neg = pd.read_csv(neg_prott5_path, header=None)\n",
    "    \n",
    "    for i in range(0, len(fasta_lines), 2):\n",
    "        # Process FASTA header\n",
    "        header = fasta_lines[i].strip()[1:]  # remove '>'\n",
    "        sequence = fasta_lines[i + 1].strip()\n",
    "        \n",
    "        # Parse header - extract entry ID between | characters and position after |-|\n",
    "        entry = extract_entry_id(header.split('|-|')[0])\n",
    "        pos = int(header.split('|-|')[1])\n",
    "        \n",
    "        # Get corresponding ProtT5 line\n",
    "        prott5_line = prott5_neg.iloc[i//2]\n",
    "        \n",
    "        # Create data entry\n",
    "        data_entry = {\n",
    "            'entry': entry,\n",
    "            'pos': pos,\n",
    "            'sequence': sequence,\n",
    "            'label': 0,\n",
    "            'has_structure': False\n",
    "        }\n",
    "        \n",
    "        # Add ProtT5 embeddings if requested\n",
    "        if not exclude_emb:\n",
    "            for j in range(2, len(prott5_line)):  # skip entry and pos columns\n",
    "                data_entry[f'emb_{j-2}'] = prott5_line[j]\n",
    "        \n",
    "        # Add structure features if available\n",
    "        if (entry, pos) in struct_dict:\n",
    "            data_entry['has_structure'] = True\n",
    "            struct_features = struct_dict[(entry, pos)]\n",
    "            for key, value in struct_features.items():\n",
    "                if key not in ['entry', 'pos', 'sequence', 'label']:\n",
    "                    data_entry[key] = value\n",
    "        \n",
    "        negative_data.append(data_entry)\n",
    "    \n",
    "    # Combine all data\n",
    "    all_data = pd.DataFrame(positive_data + negative_data)\n",
    "    \n",
    "    # Check for duplicates\n",
    "    duplicates = all_data.duplicated(subset=['entry', 'pos'], keep=False)\n",
    "    if duplicates.any():\n",
    "        print(\"\\nWarning: Found duplicate entries:\")\n",
    "        print(all_data[duplicates][['entry', 'pos', 'label']].sort_values(['entry', 'pos']))\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\nDataset statistics:\")\n",
    "    print(f\"Total entries: {len(all_data)}\")\n",
    "    print(f\"Positive examples: {len(positive_data)}\")\n",
    "    print(f\"Negative examples: {len(negative_data)}\")\n",
    "    print(f\"Entries with structure: {all_data['has_structure'].sum()}\")\n",
    "    print(f\"Unique proteins: {all_data['entry'].nunique()}\")\n",
    "    \n",
    "    # Print some example entries to verify correct parsing\n",
    "    print(\"\\nExample entries to verify parsing:\")\n",
    "    print(all_data[['entry', 'pos', 'sequence', 'label', 'has_structure']].head())\n",
    "    \n",
    "    return all_data\n",
    "\n",
    "def prepare_sequence_data(df):\n",
    "    \"\"\"Convert sequences to integer encoding\"\"\"\n",
    "    alphabet = 'ARNDCQEGHILKMFPSTWYV-'\n",
    "    char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "    \n",
    "    sequences = df['sequence'].values\n",
    "    encodings = []\n",
    "    \n",
    "    for seq in sequences:\n",
    "        try:\n",
    "            integer_encoded = [char_to_int[char] for char in seq]\n",
    "            encodings.append(integer_encoded)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sequence: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return np.array(encodings)\n",
    "\n",
    "def prepare_structure_data(df):\n",
    "    \"\"\"Enhanced feature preparation focusing on important features\"\"\"\n",
    "    features_list = []\n",
    "    middle_pos = 16  # Center position\n",
    "    \n",
    "    def normalize_angles(angle_array):\n",
    "        angle_rad = np.pi * angle_array / 180.0\n",
    "        return np.stack([np.sin(angle_rad), np.cos(angle_rad)], axis=-1)\n",
    "    \n",
    "    # Most important features (importance > 0.04)\n",
    "    important_features = [\n",
    "        'bfactor',\n",
    "        'distance_to_center',\n",
    "        'sasa',\n",
    "        'omega',\n",
    "        'domain_position'\n",
    "    ]\n",
    "    \n",
    "    for feature in important_features:\n",
    "        feature_arrays = np.array([np.array(eval(x)) for x in df[feature]])\n",
    "        # Take only the center position\n",
    "        center_values = feature_arrays[:, middle_pos]\n",
    "        scaler = RobustScaler()\n",
    "        scaled_values = scaler.fit_transform(center_values.reshape(-1, 1))\n",
    "        features_list.append(scaled_values)\n",
    "    \n",
    "    # Secondary important features (0.03 < importance < 0.04)\n",
    "    secondary_features = [\n",
    "        'chi1', 'chi2', 'chi3', 'chi4',\n",
    "        'curvature', 'psi', 'phi', 'tau',\n",
    "        'packing_density', 'local_hydrophobicity'\n",
    "    ]\n",
    "    \n",
    "    for feature in secondary_features:\n",
    "        feature_arrays = np.array([np.array(eval(x)) for x in df[feature]])\n",
    "        center_values = feature_arrays[:, middle_pos]\n",
    "        if feature in ['phi', 'psi', 'omega', 'tau']:\n",
    "            # Angle features get sin/cos encoding\n",
    "            angle_features = normalize_angles(center_values)\n",
    "            features_list.append(angle_features)\n",
    "        else:\n",
    "            scaler = RobustScaler()\n",
    "            scaled_values = scaler.fit_transform(center_values.reshape(-1, 1))\n",
    "            features_list.append(scaled_values)\n",
    "    \n",
    "    # Special case: hydrophobicity with ±1 window\n",
    "    hydro_arrays = np.array([np.array(eval(x)) for x in df['hydrophobicity']])\n",
    "    hydro_window = hydro_arrays[:, middle_pos-1:middle_pos+2]  # ±1 window\n",
    "    scaler = RobustScaler()\n",
    "    hydro_scaled = scaler.fit_transform(hydro_window.reshape(-1, 1)).reshape(len(hydro_arrays), 3)\n",
    "    features_list.append(hydro_scaled)\n",
    "    \n",
    "    # Combine all features\n",
    "    features = np.concatenate(features_list, axis=-1)\n",
    "    \n",
    "    # Add a dimension for the \"sequence\" length (1 for center position)\n",
    "    features = features.reshape(features.shape[0], 1, -1)\n",
    "    \n",
    "    return features\n",
    "\n",
    "def create_model_without_structure(seq_length=33):\n",
    "    \"\"\"Create model for samples without structure data (sequence + ProtT5 only)\"\"\"\n",
    "    # Sequence track\n",
    "    seq_input = tf.keras.layers.Input(shape=(seq_length,), name='sequence_input')\n",
    "    x_seq = tf.keras.layers.Embedding(21, 21, input_length=seq_length)(seq_input)\n",
    "    x_seq = tf.keras.layers.Reshape((seq_length, 21, 1))(x_seq)\n",
    "    x_seq = tf.keras.layers.Conv2D(32, kernel_size=(17, 3), activation='relu', padding='valid')(x_seq)\n",
    "    x_seq = tf.keras.layers.Dropout(0.2)(x_seq)\n",
    "    x_seq = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x_seq)\n",
    "    x_seq = tf.keras.layers.Flatten()(x_seq)\n",
    "    x_seq = tf.keras.layers.Dense(32, activation='relu', name='seq_features')(x_seq)\n",
    "    x_seq = tf.keras.layers.Dropout(0.2)(x_seq)\n",
    "\n",
    "    # ProtT5 track (using their architecture)\n",
    "    prot_t5_input = tf.keras.layers.Input(shape=(1024,), name='prot_t5_input')\n",
    "    x_prot_t5 = tf.keras.layers.Dense(256)(prot_t5_input)\n",
    "    x_prot_t5 = tf.keras.layers.Dropout(0.4)(x_prot_t5)\n",
    "    x_prot_t5 = tf.keras.layers.Dense(128, activation='relu')(x_prot_t5)\n",
    "    x_prot_t5 = tf.keras.layers.Dropout(0.4)(x_prot_t5)\n",
    "\n",
    "    # Create learnable weights layer for two tracks\n",
    "    weight_layer = tf.keras.layers.Dense(2, activation='softmax', name='track_weights')\n",
    "    track_weights = weight_layer(tf.keras.layers.Concatenate()([x_seq, x_prot_t5]))\n",
    "\n",
    "    # Apply weights\n",
    "    weighted_seq = tf.keras.layers.Multiply(name='weighted_seq')([\n",
    "        x_seq,\n",
    "        tf.keras.layers.Lambda(lambda x: x[:, 0:1])(track_weights)\n",
    "    ])\n",
    "    \n",
    "    weighted_prot_t5 = tf.keras.layers.Multiply(name='weighted_prot_t5')([\n",
    "        x_prot_t5,\n",
    "        tf.keras.layers.Lambda(lambda x: x[:, 1:2])(track_weights)\n",
    "    ])\n",
    "\n",
    "    # Combine features\n",
    "    combined = tf.keras.layers.Concatenate()([weighted_seq, weighted_prot_t5])\n",
    "\n",
    "    # Final layers\n",
    "    x = tf.keras.layers.Dense(32, activation='relu')(combined)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[seq_input, prot_t5_input], outputs=outputs)\n",
    "\n",
    "    def get_track_weights():\n",
    "        w = model.get_layer('track_weights').get_weights()\n",
    "        if len(w) > 0:\n",
    "            weights = w[0]\n",
    "            bias = w[1] if len(w) > 1 else 0\n",
    "            exp_weights = np.exp(np.mean(weights, axis=0) + bias)\n",
    "            normalized = exp_weights / np.sum(exp_weights)\n",
    "            return {\n",
    "                'sequence': float(normalized[0]),\n",
    "                'prot_t5': float(normalized[1])\n",
    "            }\n",
    "        return {'sequence': 0.5, 'prot_t5': 0.5}\n",
    "\n",
    "    model.get_track_weights = get_track_weights\n",
    "    return model\n",
    "\n",
    "def create_combined_model_with_prot_t5(seq_length=33, struct_features=None, struct_window=0):\n",
    "    regularizer = tf.keras.regularizers.l2(0.01)\n",
    "    \n",
    "    # Sequence track\n",
    "    seq_input = tf.keras.layers.Input(shape=(seq_length,), name='sequence_input')\n",
    "    x_seq = tf.keras.layers.Embedding(21, 21, input_length=seq_length)(seq_input)\n",
    "    x_seq = tf.keras.layers.Reshape((seq_length, 21, 1))(x_seq)\n",
    "    x_seq = tf.keras.layers.Conv2D(32, kernel_size=(17, 3), activation='relu', padding='valid')(x_seq)\n",
    "    x_seq = tf.keras.layers.BatchNormalization()(x_seq)\n",
    "    x_seq = tf.keras.layers.Dropout(0.4)(x_seq)\n",
    "    x_seq = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x_seq)\n",
    "    x_seq = tf.keras.layers.Flatten()(x_seq)\n",
    "    x_seq = tf.keras.layers.Dense(32, activation='relu', \n",
    "                                 kernel_regularizer=regularizer, \n",
    "                                 name='seq_features')(x_seq)\n",
    "    x_seq = tf.keras.layers.BatchNormalization()(x_seq)\n",
    "    x_seq = tf.keras.layers.Dropout(0.4)(x_seq)\n",
    "\n",
    "    # Structure track\n",
    "    struct_input = tf.keras.layers.Input(shape=(1, struct_features), name='structure_input')\n",
    "    x_struct = struct_input\n",
    "    struct_dense_size = min(struct_features * 2, 128)\n",
    "    \n",
    "    x_struct = tf.keras.layers.Conv1D(32, 3, padding='same', activation='relu')(x_struct)\n",
    "    x_struct = tf.keras.layers.BatchNormalization()(x_struct)\n",
    "    x_struct = tf.keras.layers.Dropout(0.4)(x_struct)\n",
    "    x_struct = tf.keras.layers.Flatten()(x_struct)\n",
    "    x_struct = tf.keras.layers.Dense(struct_dense_size, activation='relu',\n",
    "                                   kernel_regularizer=regularizer)(x_struct)\n",
    "    x_struct = tf.keras.layers.BatchNormalization()(x_struct)\n",
    "    x_struct = tf.keras.layers.Dropout(0.4)(x_struct)\n",
    "\n",
    "    # ProtT5 track\n",
    "    prot_t5_input = tf.keras.layers.Input(shape=(1024,), name='prot_t5_input')\n",
    "    x_prot_t5 = tf.keras.layers.Dense(256, kernel_regularizer=regularizer)(prot_t5_input)\n",
    "    x_prot_t5 = tf.keras.layers.BatchNormalization()(x_prot_t5)\n",
    "    x_prot_t5 = tf.keras.layers.Dropout(0.5)(x_prot_t5)\n",
    "    x_prot_t5 = tf.keras.layers.Dense(128, activation='relu',\n",
    "                                     kernel_regularizer=regularizer)(x_prot_t5)\n",
    "    x_prot_t5 = tf.keras.layers.BatchNormalization()(x_prot_t5)\n",
    "    x_prot_t5 = tf.keras.layers.Dropout(0.5)(x_prot_t5)\n",
    "\n",
    "    # Create learnable weights layer\n",
    "    weight_layer = tf.keras.layers.Dense(3, activation='softmax', name='track_weights')\n",
    "    track_weights = weight_layer(tf.keras.layers.Concatenate()([x_seq, x_struct, x_prot_t5]))\n",
    "\n",
    "    # Apply weights\n",
    "    weighted_seq = tf.keras.layers.Multiply(name='weighted_seq')([\n",
    "        x_seq,\n",
    "        tf.keras.layers.Lambda(lambda x: x[:, 0:1])(track_weights)\n",
    "    ])\n",
    "    \n",
    "    weighted_struct = tf.keras.layers.Multiply(name='weighted_struct')([\n",
    "        x_struct,\n",
    "        tf.keras.layers.Lambda(lambda x: x[:, 1:2])(track_weights)\n",
    "    ])\n",
    "    \n",
    "    weighted_prot_t5 = tf.keras.layers.Multiply(name='weighted_prot_t5')([\n",
    "        x_prot_t5,\n",
    "        tf.keras.layers.Lambda(lambda x: x[:, 2:3])(track_weights)\n",
    "    ])\n",
    "\n",
    "    # Combine features\n",
    "    combined = tf.keras.layers.Concatenate()([weighted_seq, weighted_struct, weighted_prot_t5])\n",
    "\n",
    "    # Final layers with more regularization\n",
    "    x = tf.keras.layers.Dense(64, activation='relu', \n",
    "                            kernel_regularizer=regularizer)(combined)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    x = tf.keras.layers.Dense(32, activation='relu', \n",
    "                            kernel_regularizer=regularizer)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[seq_input, struct_input, prot_t5_input], outputs=outputs)\n",
    "    \n",
    "    def get_track_weights():\n",
    "        w = model.get_layer('track_weights').get_weights()\n",
    "        if len(w) > 0:\n",
    "            weights = w[0]\n",
    "            bias = w[1] if len(w) > 1 else 0\n",
    "            exp_weights = np.exp(np.mean(weights, axis=0) + bias)\n",
    "            normalized = exp_weights / np.sum(exp_weights)\n",
    "            return {\n",
    "                'sequence': float(normalized[0]),\n",
    "                'structure': float(normalized[1]),\n",
    "                'prot_t5': float(normalized[2])\n",
    "            }\n",
    "        return {'sequence': 0.33, 'structure': 0.33, 'prot_t5': 0.34}\n",
    "\n",
    "    model.get_track_weights = get_track_weights\n",
    "    return model\n",
    "\n",
    "def print_final_results(track_weights_history, metrics, test_predictions, y_test):\n",
    "    \"\"\"Print final results and statistics\"\"\"\n",
    "    # Calculate average weights\n",
    "    avg_seq_weight = np.mean([w['sequence'] for w in track_weights_history])\n",
    "    avg_struct_weight = np.mean([w['structure'] for w in track_weights_history])\n",
    "    avg_prot_t5_weight = np.mean([w['prot_t5'] for w in track_weights_history])\n",
    "    std_seq_weight = np.std([w['sequence'] for w in track_weights_history])\n",
    "    std_struct_weight = np.std([w['structure'] for w in track_weights_history])\n",
    "    std_prot_t5_weight = np.std([w['prot_t5'] for w in track_weights_history])\n",
    "    \n",
    "    print(\"\\nAverage track weights across folds:\")\n",
    "    print(f\"Sequence weight: {avg_seq_weight:.4f} ± {std_seq_weight:.4f}\")\n",
    "    print(f\"Structure weight: {avg_struct_weight:.4f} ± {std_struct_weight:.4f}\")\n",
    "    print(f\"ProtT5 weight: {avg_prot_t5_weight:.4f} ± {std_prot_t5_weight:.4f}\")\n",
    "    \n",
    "    # Print average cross-validation results\n",
    "    print(\"\\nAverage Cross-validation Results:\")\n",
    "    for metric in metrics:\n",
    "        print(f\"{metric.upper()}: {np.mean(metrics[metric]):.4f} ± {np.std(metrics[metric]):.4f}\")\n",
    "    \n",
    "    # Calculate final test results\n",
    "    test_pred_avg = np.mean(test_predictions, axis=0)\n",
    "    test_pred_binary = (test_pred_avg > 0.5).astype(int)\n",
    "    cm_test = confusion_matrix(y_test, test_pred_binary)\n",
    "    \n",
    "    print(\"\\nFinal Test Set Results:\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, test_pred_binary):.4f}\")\n",
    "    print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, test_pred_binary):.4f}\")\n",
    "    print(f\"MCC: {matthews_corrcoef(y_test, test_pred_binary):.4f}\")\n",
    "    print(f\"Sensitivity: {cm_test[1][1]/(cm_test[1][1]+cm_test[1][0]):.4f}\")\n",
    "    print(f\"Specificity: {cm_test[0][0]/(cm_test[0][0]+cm_test[0][1]):.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm_test)\n",
    "    \n",
    "def train_and_evaluate_split():\n",
    "    \"\"\"Training function that handles both cases with k-fold validation\"\"\"\n",
    "    # Load complete data without embeddings\n",
    "    print(\"Loading complete data...\")\n",
    "    train_data = load_complete_data(mode='train', exclude_emb=True)\n",
    "    test_data = load_complete_data(mode='test', exclude_emb=True)\n",
    "    \n",
    "    # Load ProtT5 embeddings\n",
    "    print(\"Loading ProtT5 embeddings...\")\n",
    "    train_pos_dict, train_neg_dict = load_prot_t5_data(\n",
    "        '/Users/hai/Workspace/UniFreiburg/WS2024/Thesis/LMSuccSite/data/train/features/train_positive_ProtT5-XL-UniRef50.csv',\n",
    "        '/Users/hai/Workspace/UniFreiburg/WS2024/Thesis/LMSuccSite/data/train/features/train_negative_ProtT5-XL-UniRef50.csv'\n",
    "    )\n",
    "    test_pos_dict, test_neg_dict = load_prot_t5_data(\n",
    "        '/Users/hai/Workspace/UniFreiburg/WS2024/Thesis/LMSuccSite/data/test/features/test_positive_ProtT5-XL-UniRef50.csv',\n",
    "        '/Users/hai/Workspace/UniFreiburg/WS2024/Thesis/LMSuccSite/data/test/features/test_negative_ProtT5-XL-UniRef50.csv'\n",
    "    )\n",
    "    \n",
    "    # Align data with ProtT5 embeddings\n",
    "    print(\"Aligning data with ProtT5 embeddings...\")\n",
    "    X_train_prot_t5, train_data_aligned = prepare_aligned_data(train_data, train_pos_dict, train_neg_dict)\n",
    "    X_test_prot_t5, test_data_aligned = prepare_aligned_data(test_data, test_pos_dict, test_neg_dict)\n",
    "    \n",
    "    # Split data by structure availability\n",
    "    train_with_struct = train_data_aligned[train_data_aligned['has_structure']].copy()\n",
    "    train_without_struct = train_data_aligned[~train_data_aligned['has_structure']].copy()\n",
    "    test_with_struct = test_data_aligned[test_data_aligned['has_structure']].copy()\n",
    "    test_without_struct = test_data_aligned[~test_data_aligned['has_structure']].copy()\n",
    "    \n",
    "    print(\"\\nData Statistics:\")\n",
    "    print(f\"Total training samples: {len(train_data_aligned)}\")\n",
    "    print(f\"Training samples with structure: {len(train_with_struct)} ({len(train_with_struct)/len(train_data_aligned)*100:.1f}%)\")\n",
    "    print(f\"Total test samples: {len(test_data_aligned)}\")\n",
    "    print(f\"Test samples with structure: {len(test_with_struct)} ({len(test_with_struct)/len(test_data_aligned)*100:.1f}%)\")\n",
    "    \n",
    "    # Prepare sequence data for all samples\n",
    "    X_train_seq_all = prepare_sequence_data(train_data_aligned)\n",
    "    X_test_seq_all = prepare_sequence_data(test_data_aligned)\n",
    "    y_train_all = train_data_aligned['label'].values\n",
    "    y_test_all = test_data_aligned['label'].values\n",
    "    \n",
    "    # Prepare data for samples with structure\n",
    "    if len(train_with_struct) > 0:\n",
    "        X_train_seq_struct = prepare_sequence_data(train_with_struct)\n",
    "        X_train_struct = prepare_structure_data(train_with_struct)\n",
    "        X_train_prot_t5_struct = X_train_prot_t5[train_data_aligned['has_structure']]\n",
    "        y_train_struct = train_with_struct['label'].values\n",
    "        \n",
    "        X_test_seq_struct = prepare_sequence_data(test_with_struct)\n",
    "        X_test_struct = prepare_structure_data(test_with_struct)\n",
    "        X_test_prot_t5_struct = X_test_prot_t5[test_data_aligned['has_structure']]\n",
    "        y_test_struct = test_with_struct['label'].values\n",
    "    \n",
    "    # Prepare data for samples without structure\n",
    "    X_train_seq_no_struct = prepare_sequence_data(train_without_struct)\n",
    "    X_train_prot_t5_no_struct = X_train_prot_t5[~train_data_aligned['has_structure']]\n",
    "    y_train_no_struct = train_without_struct['label'].values\n",
    "    \n",
    "    X_test_seq_no_struct = prepare_sequence_data(test_without_struct)\n",
    "    X_test_prot_t5_no_struct = X_test_prot_t5[~test_data_aligned['has_structure']]\n",
    "    y_test_no_struct = test_without_struct['label'].values\n",
    "    \n",
    "    # Calculate class weights\n",
    "    total_samples = len(y_train_all)\n",
    "    pos_samples = np.sum(y_train_all == 1)\n",
    "    neg_samples = np.sum(y_train_all == 0)\n",
    "    class_weights = {\n",
    "        0: total_samples / (2 * neg_samples),\n",
    "        1: total_samples / (2 * pos_samples)\n",
    "    }\n",
    "    \n",
    "    # Print class distribution\n",
    "    print(\"\\nTraining set distribution:\")\n",
    "    print(pd.Series(y_train_all).value_counts())\n",
    "    print(\"\\nTest set distribution:\")\n",
    "    print(pd.Series(y_test_all).value_counts())\n",
    "    \n",
    "    # Print data shapes\n",
    "    print(\"\\nData shapes:\")\n",
    "    print(f\"X_train_seq_all: {X_train_seq_all.shape}\")\n",
    "    print(f\"X_train_prot_t5: {X_train_prot_t5.shape}\")\n",
    "    if len(train_with_struct) > 0:\n",
    "        print(f\"X_train_struct: {X_train_struct.shape}\")\n",
    "    \n",
    "    # Initialize k-fold\n",
    "    kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    metrics = {'acc': [], 'balanced_acc': [], 'mcc': [], 'sn': [], 'sp': []}\n",
    "    test_predictions = []\n",
    "    track_weights_history = []\n",
    "    \n",
    "    # Training callbacks\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=3,\n",
    "            min_lr=1e-6\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train_seq_all), 1):\n",
    "        print(f\"\\nFold {fold}/5\")\n",
    "        \n",
    "        # Train sequence+ProtT5 model on all data\n",
    "        print(\"\\nTraining sequence+ProtT5 model...\")\n",
    "        model_no_struct = create_model_without_structure(seq_length=33)\n",
    "        model_no_struct.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        history_no_struct = model_no_struct.fit(\n",
    "            [X_train_seq_all[train_idx], X_train_prot_t5[train_idx]],\n",
    "            y_train_all[train_idx],\n",
    "            validation_data=(\n",
    "                [X_train_seq_all[val_idx], X_train_prot_t5[val_idx]],\n",
    "                y_train_all[val_idx]\n",
    "            ),\n",
    "            batch_size=32,\n",
    "            epochs=50,\n",
    "            callbacks=callbacks,\n",
    "            class_weight=class_weights,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Train full model with structure\n",
    "        if len(train_with_struct) > 0:\n",
    "            print(\"\\nTraining full model...\")\n",
    "            # Find indices for structure data\n",
    "            struct_mask = train_data_aligned['has_structure'].values\n",
    "            struct_train_idx = train_idx[struct_mask[train_idx]]\n",
    "            struct_val_idx = val_idx[struct_mask[val_idx]]\n",
    "            \n",
    "            model_with_struct = create_combined_model_with_prot_t5(\n",
    "                seq_length=33,\n",
    "                struct_features=X_train_struct.shape[2],\n",
    "                struct_window=0\n",
    "            )\n",
    "            \n",
    "            model_with_struct.compile(\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "            \n",
    "            history_struct = model_with_struct.fit(\n",
    "                [X_train_seq_struct[struct_train_idx], X_train_struct[struct_train_idx], X_train_prot_t5_struct[struct_train_idx]],\n",
    "                y_train_struct[struct_train_idx],\n",
    "                validation_data=(\n",
    "                    [X_train_seq_struct[struct_val_idx], X_train_struct[struct_val_idx], X_train_prot_t5_struct[struct_val_idx]],\n",
    "                    y_train_struct[struct_val_idx]\n",
    "                ),\n",
    "                batch_size=32,\n",
    "                epochs=50,\n",
    "                callbacks=callbacks,\n",
    "                class_weight=class_weights,\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            # Store track weights\n",
    "            track_weights = model_with_struct.get_track_weights()\n",
    "            track_weights_history.append(track_weights)\n",
    "            print(f\"\\nLearned track weights for fold {fold}:\")\n",
    "            print(f\"Sequence weight: {track_weights['sequence']:.4f}\")\n",
    "            print(f\"Structure weight: {track_weights['structure']:.4f}\")\n",
    "            print(f\"ProtT5 weight: {track_weights['prot_t5']:.4f}\")\n",
    "        \n",
    "        # Plot training histories\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        # Plot sequence+ProtT5 model history\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history_no_struct.history['accuracy'], label='Train')\n",
    "        plt.plot(history_no_struct.history['val_accuracy'], label='Val')\n",
    "        plt.title(f'Seq+ProtT5 Model - Fold {fold}')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Plot full model history if available\n",
    "        if len(train_with_struct) > 0:\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot(history_struct.history['accuracy'], label='Train')\n",
    "            plt.plot(history_struct.history['val_accuracy'], label='Val')\n",
    "            plt.title(f'Full Model - Fold {fold}')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Get predictions for test set\n",
    "        fold_predictions = []\n",
    "        fold_labels = []\n",
    "        \n",
    "        # Predictions for samples with structure\n",
    "        if len(test_with_struct) > 0:\n",
    "            pred_struct = model_with_struct.predict(\n",
    "                [X_test_seq_struct, X_test_struct, X_test_prot_t5_struct]\n",
    "            )\n",
    "            fold_predictions.extend(pred_struct)\n",
    "            fold_labels.extend(y_test_struct)\n",
    "        \n",
    "        # Predictions for samples without structure\n",
    "        if len(test_without_struct) > 0:\n",
    "            pred_no_struct = model_no_struct.predict(\n",
    "                [X_test_seq_no_struct, X_test_prot_t5_no_struct]\n",
    "            )\n",
    "            fold_predictions.extend(pred_no_struct)\n",
    "            fold_labels.extend(y_test_no_struct)\n",
    "        \n",
    "        # Convert to numpy arrays\n",
    "        fold_predictions = np.array(fold_predictions)\n",
    "        fold_labels = np.array(fold_labels)\n",
    "        \n",
    "        # Calculate metrics for this fold\n",
    "        fold_pred_binary = (fold_predictions > 0.5).astype(int)\n",
    "        cm = confusion_matrix(fold_labels, fold_pred_binary)\n",
    "        \n",
    "        metrics['acc'].append(accuracy_score(fold_labels, fold_pred_binary))\n",
    "        metrics['balanced_acc'].append(balanced_accuracy_score(fold_labels, fold_pred_binary))\n",
    "        metrics['mcc'].append(matthews_corrcoef(fold_labels, fold_pred_binary))\n",
    "        metrics['sn'].append(cm[1][1]/(cm[1][1]+cm[1][0]))  # Sensitivity\n",
    "        metrics['sp'].append(cm[0][0]/(cm[0][0]+cm[0][1]))  # Specificity\n",
    "        \n",
    "        # Store predictions\n",
    "        test_predictions.append(fold_predictions)\n",
    "        \n",
    "        # Print fold results\n",
    "        print(f\"\\nFold {fold} Results:\")\n",
    "        print(f\"Accuracy: {metrics['acc'][-1]:.4f}\")\n",
    "        print(f\"Balanced Accuracy: {metrics['balanced_acc'][-1]:.4f}\")\n",
    "        print(f\"MCC: {metrics['mcc'][-1]:.4f}\")\n",
    "        print(f\"Sensitivity: {metrics['sn'][-1]:.4f}\")\n",
    "        print(f\"Specificity: {metrics['sp'][-1]:.4f}\")\n",
    "    \n",
    "    # Calculate and print final results\n",
    "    test_pred_avg = np.mean(test_predictions, axis=0)\n",
    "    test_pred_binary = (test_pred_avg > 0.5).astype(int)\n",
    "    cm_test = confusion_matrix(y_test_all, test_pred_binary)\n",
    "    \n",
    "    print(\"\\nAverage Cross-validation Results:\")\n",
    "    for metric in metrics:\n",
    "        print(f\"{metric.upper()}: {np.mean(metrics[metric]):.4f} ± {np.std(metrics[metric]):.4f}\")\n",
    "    \n",
    "    print(\"\\nFinal Test Set Results:\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test_all, test_pred_binary):.4f}\")\n",
    "    print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test_all, test_pred_binary):.4f}\")\n",
    "    print(f\"MCC: {matthews_corrcoef(y_test_all, test_pred_binary):.4f}\")\n",
    "    print(f\"Sensitivity: {cm_test[1][1]/(cm_test[1][1]+cm_test[1][0]):.4f}\")\n",
    "    print(f\"Specificity: {cm_test[0][0]/(cm_test[0][0]+cm_test[0][1]):.4f}\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm_test)\n",
    "    \n",
    "    if track_weights_history:\n",
    "        # Print average track weights\n",
    "        print(\"\\nAverage track weights across folds:\")\n",
    "        avg_weights = {\n",
    "            'sequence': np.mean([w['sequence'] for w in track_weights_history]),\n",
    "            'structure': np.mean([w['structure'] for w in track_weights_history]),\n",
    "            'prot_t5': np.mean([w['prot_t5'] for w in track_weights_history])\n",
    "        }\n",
    "        std_weights = {\n",
    "            'sequence': np.std([w['sequence'] for w in track_weights_history]),\n",
    "            'structure': np.std([w['structure'] for w in track_weights_history]),\n",
    "            'prot_t5': np.std([w['prot_t5'] for w in track_weights_history])\n",
    "        }\n",
    "        \n",
    "        print(f\"Sequence weight: {avg_weights['sequence']:.4f} ± {std_weights['sequence']:.4f}\")\n",
    "        print(f\"Structure weight: {avg_weights['structure']:.4f} ± {std_weights['structure']:.4f}\")\n",
    "        print(f\"ProtT5 weight: {avg_weights['prot_t5']:.4f} ± {std_weights['prot_t5']:.4f}\")\n",
    "    \n",
    "    return model_with_struct, model_no_struct\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_with_struct, model_no_struct = train_and_evaluate_split()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioinf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
